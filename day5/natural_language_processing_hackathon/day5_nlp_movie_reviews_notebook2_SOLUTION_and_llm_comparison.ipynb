{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUEoS49PbBK1ZaDfydZrpJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrincetonUniversity/intro_machine_learning/blob/main/day5/natural_language_processing_hackathon/day5_nlp_movie_reviews_notebook2_SOLUTION_and_llm_comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to Machine Learning  \n",
        "**Natural Language Processing Hackathon: Hackathon Solution  \n",
        "Wintersession 2023  \n",
        "Tuesday, January 24, 2023**"
      ],
      "metadata": {
        "id": "8YCte8fp2XDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The material here is based on Chapter 8 of \n",
        "Machine Learning with PyTorch and Scikit-Learn by Sebastian Raschka, Yuxi (Hayden) Liu, Vahid Mirjalili and Dmytro Dzhulgakov. The book is available via the PU library.\n",
        "\n",
        "In this notebook we are going to work with a dataset of 50,000 movie reviews from the Internet Movie Database (IMDb) and build a predictor that can distinguish between positive and negative reviews."
      ],
      "metadata": {
        "id": "j5YLQlOs2jNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import textwrap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "UuDdLpWUaBRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download Data and Make Dataframe"
      ],
      "metadata": {
        "id": "3Kpy-8Ff8ZZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the data:"
      ],
      "metadata": {
        "id": "wjO7F84nz99c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoSng-U6VyvC"
      },
      "outputs": [],
      "source": [
        "!wget https://tigress-web.princeton.edu/~jdh4/movie_data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in the CSV file and print the first 5 rows of the Pandas dataframe:"
      ],
      "metadata": {
        "id": "peptRcYAdrSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "DuYihEqqcBwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"raw-review\"] = df[\"review\"]"
      ],
      "metadata": {
        "id": "wp-mY7nDFfsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_html_tags(text):\n",
        "    \"\"\"Remove html tags from a string\"\"\"\n",
        "    import re\n",
        "    clean = re.compile('<.*?>')\n",
        "    return re.sub(clean, '', text)"
      ],
      "metadata": {
        "id": "QQT-KU0-GflF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remove_html_tags('What is <b>this</b>, said the toad? Where is <p class=\"new\">the time</a> probe?')"
      ],
      "metadata": {
        "id": "ZHKvtFBJHuPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"raw-review\"] = df[\"raw-review\"].apply(remove_html_tags)"
      ],
      "metadata": {
        "id": "jrOrrMkrGktN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the value of idx to vary that amount of train and test data. The default value is 25000 or a 50/50 split."
      ],
      "metadata": {
        "id": "yN0XyTfcggrf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing and Train-Test Split"
      ],
      "metadata": {
        "id": "VjfO0RH38o6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
        "    return text"
      ],
      "metadata": {
        "id": "YSqs-9TYhKt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['review'] = df['review'].apply(preprocessor)"
      ],
      "metadata": {
        "id": "hDEbzfOahQOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 25000\n",
        "X_train = df.loc[:idx - 1, 'review'].values\n",
        "y_train = df.loc[:idx - 1, 'sentiment'].values\n",
        "X_test  = df.loc[idx:, 'review'].values\n",
        "y_test  = df.loc[idx:, 'sentiment'].values"
      ],
      "metadata": {
        "id": "kOOBt1t4ccFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(text):\n",
        "    return text.split()"
      ],
      "metadata": {
        "id": "L6zxfzFjhlhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]"
      ],
      "metadata": {
        "id": "epJ9DjT31bp2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop = stopwords.words(\"english\")"
      ],
      "metadata": {
        "id": "9bee9sBr1DqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing and Training Pipeline"
      ],
      "metadata": {
        "id": "Qui887XB8GFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
        "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop],\n",
        "               'vect__tokenizer': [tokenizer],\n",
        "               'vect__use_idf': [True],\n",
        "               'vect__norm': [None],\n",
        "               'clf__penalty': ['l2'],\n",
        "               'clf__C': [1.0]}]\n",
        "\n",
        "lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(solver='liblinear'))])\n",
        "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
        "gs_lr_tfidf.fit(X_train, y_train)\n",
        "\n",
        "print(gs_lr_tfidf.best_params_)\n",
        "print(gs_lr_tfidf.best_score_)\n",
        "\n",
        "clf = gs_lr_tfidf.best_estimator_\n",
        "print('Accuracy (test):', clf.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "ewiIlHtw1I-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pipelines can be expensive to evaulate. In the above, the param_grid is chosen with one set of parameters. For a more extensive search use the param_grid below:"
      ],
      "metadata": {
        "id": "GR1Dnz2uu-0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = [{'vect__ngram_range': [(1, 3)],\n",
        "               'vect__stop_words': [None],\n",
        "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
        "               'clf__penalty': ['l2'],\n",
        "               'clf__C': [1.0, 10.0]},\n",
        "              {'vect__ngram_range': [(1, 1)],\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'vect__tokenizer': [tokenizer],\n",
        "               'vect__use_idf': [True, False],\n",
        "               'vect__norm': [None],\n",
        "               'clf__penalty': ['l2'],\n",
        "               'clf__C': [1.0, 10.0]}]"
      ],
      "metadata": {
        "id": "qv5C1CjT9c06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pretrained Large Language Model"
      ],
      "metadata": {
        "id": "sDg1yJd6-h0O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For an introduction to transformers see the Colab notebook: https://tinyurl.com/hugfacetutorial"
      ],
      "metadata": {
        "id": "nDeq3gWJ-_yx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For an introduction to transformers on the Princeton Research Computing clusters see this repo by David Turner of PNI: [GitHub](https://github.com/davidt0x/hf_tutorial). In particular, see slides.pptx"
      ],
      "metadata": {
        "id": "aY2cunOo5dsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "xpuO1dXl_Xv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "sentiment_pipeline = pipeline('text-classification', model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
      ],
      "metadata": {
        "id": "kusC65__-mhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = df.loc[0]['raw-review']\n",
        "print(review)"
      ],
      "metadata": {
        "id": "gVlvEZHKAHpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipeline(review)[0]['label']"
      ],
      "metadata": {
        "id": "SyVYEWCj-nWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"truncated-review\"] = df['raw-review'].apply(lambda x: x if len(x.split()) < 300 else ' '.join(x.split()[:300]))"
      ],
      "metadata": {
        "id": "kYPONnP4AsHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub = df[:250].copy()"
      ],
      "metadata": {
        "id": "SCGNiTMiCzT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub.head()"
      ],
      "metadata": {
        "id": "BMW4l4_lDNfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub[\"pretrained-distillbert-pred\"] = df_sub['truncated-review'].apply(lambda x: sentiment_pipeline(x)[0]['label'])"
      ],
      "metadata": {
        "id": "E8vu6XR2APgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub[\"pretrained-distillbert-pred\"].value_counts()"
      ],
      "metadata": {
        "id": "R9ePJ_pPIMQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sub[\"pretrained-distillbert-pred\"] = df_sub[\"pretrained-distillbert-pred\"].apply(lambda x: 0 if x == 'NEGATIVE' else 1)"
      ],
      "metadata": {
        "id": "HuHm-pbXAhjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distillbert_accuracy = df_sub[df_sub[\"pretrained-distillbert-pred\"] == df_sub[\"sentiment\"]].shape[0] / df_sub.shape[0]\n",
        "print(f'{100 * distillbert_accuracy}%')"
      ],
      "metadata": {
        "id": "K5NjrRDTEjdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get almost the same accuracy but with no training from the LLM versus our ML model."
      ],
      "metadata": {
        "id": "O_A6J81GJnzW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CufPereWLmBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: Use the LLM to summarize one of the reviews."
      ],
      "metadata": {
        "id": "F5A0z32oLmxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarization_pipeline = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")"
      ],
      "metadata": {
        "id": "z02Fqpw8Lsjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "review = df.loc[6][\"raw-review\"]\n",
        "review"
      ],
      "metadata": {
        "id": "svuOlXQDL3yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = summarization_pipeline(review, max_length=80, clean_up_tokenization_spaces=True)\n",
        "wrapper = textwrap.TextWrapper(width=80, break_long_words=False, break_on_hyphens=False)\n",
        "print(wrapper.fill(outputs[0]['summary_text']))"
      ],
      "metadata": {
        "id": "HP6fyqseLvIh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPEd9Oono0VQxWVnVLgHzvt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrincetonUniversity/intro_machine_learning/blob/main/day5/natural_language_processing_hackathon/day5_nlp_movie_reviews_notebook1_bag_of_words.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to Machine Learning  \n",
        "**Natural Language Processing Hackathon: Notebook 1  \n",
        "Wintersession  \n",
        "Tuesday, January 24, 2023**"
      ],
      "metadata": {
        "id": "MH7MrrKyZ3dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The material here is based on Chapter 8 of \n",
        "Machine Learning with PyTorch and Scikit-Learn by Sebastian Raschka, Yuxi (Hayden) Liu, Vahid Mirjalili and Dmytro Dzhulgakov. The book is available via the PU library."
      ],
      "metadata": {
        "id": "VkcS5HW9VrsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pprint\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "metadata": {
        "id": "UuDdLpWUaBRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to process natural language using a computer?"
      ],
      "metadata": {
        "id": "HnX9D9Zta3Og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our focus for this project will be sentiment analysis or opinion mining. That is, for a given document, is the sentiment or tone of the document positive or negative?"
      ],
      "metadata": {
        "id": "xRIImSCma80-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"best movie ever\"  \n",
        "\"we found this movie to be very entertaining\"  \n",
        "\"this movie was the worst movie ever\"  "
      ],
      "metadata": {
        "id": "vOA93jYdbLh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use computers to do natural language processing we need to convert the text to numbers. What simple approaches can one think of to do this?"
      ],
      "metadata": {
        "id": "qFB1aTjYbqa0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words"
      ],
      "metadata": {
        "id": "X0ilCceydSks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One approach is to count the number of times that each word appears in each document and associate these counts with the class label. This approach is called bag of words. Let's look at an example."
      ],
      "metadata": {
        "id": "uqMVGSXIdVyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"review\":[\"best movie ever\",\n",
        "                             \"we found this movie to be very entertaining\",\n",
        "                             \"this movie was the worst movie ever\"],\n",
        "                   \"sentimemt\":[1, 1, 0]})\n",
        "df"
      ],
      "metadata": {
        "id": "dCiRWIxOeJqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use a tool called a CountVectorizer to perform the counting. See the documentation for the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
      ],
      "metadata": {
        "id": "y9Ri8YtPn5A8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = CountVectorizer(stop_words=None)\n",
        "bag = count.fit_transform(df[\"review\"])"
      ],
      "metadata": {
        "id": "YJmG6rmSeys6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataframe below shows the term frequencies for each review:"
      ],
      "metadata": {
        "id": "fIwstwEokz5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = pd.DataFrame(bag.toarray())\n",
        "numbers.columns = sorted(count.vocabulary_.keys())\n",
        "numbers"
      ],
      "metadata": {
        "id": "NnwITsTXkkOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have features that can be used for training a machine learning model! Let's add a few more pieces."
      ],
      "metadata": {
        "id": "iVuJgsA3hZSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Term Frequency-Inverse Document Frequency"
      ],
      "metadata": {
        "id": "33dRb1HPhk1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some words appear in many of the reviews (or documents in general) while others only appear rarely. Let's come up with a scheme for up-weighting the rare words and down-weighting the common words. Our hypothesis is that the rare words have more importance.\n",
        "\n",
        "One solution is to multiply the term frequency of a given word in a document by the log of the ratio of the number of documents divided by the number of documents containing that word. Like this:\n",
        "\n",
        "tf(w, r) = count of word w in review r  \n",
        "N = total number of reviews  \n",
        "n(w) = number of reviews containing word w  \n",
        "\n",
        "\n",
        "tf-idf = tf(w, r) log ((N + 1) / (n(w) + 1))\n",
        "\n",
        "The log of the ratio is used to prevent very rare words from getting excess weight. Let's try it out and see it the results make sense."
      ],
      "metadata": {
        "id": "RQka29bvhpiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
        "tbl = tfidf.fit_transform(bag).toarray()\n",
        "numbers = pd.DataFrame(tbl)\n",
        "numbers.columns = sorted(count.vocabulary_.keys())\n",
        "numbers.round(decimals=2)"
      ],
      "metadata": {
        "id": "A0P7ehxOlcrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the first row above, \"best\" has the largest value. This makes sense since it only appears once in that review and not in others. The word \"movie\" appears in all reviews and its magnitude is smallest. In the third row, \"movie\" has the largest magnitude despite being a common word. This arises because appears twice so its term frequency is 2 which is high."
      ],
      "metadata": {
        "id": "NEq1PUAAXxss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The values in the table above have been normalized by row. Let's check that each row is normalized:"
      ],
      "metadata": {
        "id": "oPIOrfa-ngsZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print([np.linalg.norm(tbl[i]) for i in [0, 1, 2]])"
      ],
      "metadata": {
        "id": "ubOEKze7mgC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that using use_idf=False, norm=None and smooth_idf=False simply gives the word counts:"
      ],
      "metadata": {
        "id": "W3knL9XOpr29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfTransformer(use_idf=False, norm=None, smooth_idf=False)\n",
        "print(tfidf.fit_transform(bag).toarray())"
      ],
      "metadata": {
        "id": "Gz7fzwgIpgja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "aM0JA2CAqRiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Words like running and run are closely related. They derive from the same stem. We can reduce the number of words by applying stemming."
      ],
      "metadata": {
        "id": "V_RmC88kqY-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]\n",
        "tokenizer_porter('runners like running and thus they run')"
      ],
      "metadata": {
        "id": "FzaRtOJYsESj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is also the trivial tokenizer which does not perform stemming:"
      ],
      "metadata": {
        "id": "y5oKhZgRtuQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "tokenizer('runners like running and thus they run')\n",
        "['runners', 'like', 'running', 'and', 'thus', 'they', 'run']"
      ],
      "metadata": {
        "id": "aBUCpFhVtyYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Cleaning"
      ],
      "metadata": {
        "id": "99sxBuYuunAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
        "    return text"
      ],
      "metadata": {
        "id": "MXsMVM4wuqEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor(\"</a>This :) is :( a test :-)!\")"
      ],
      "metadata": {
        "id": "v4F-q3Xvurxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Via the first regex, <[^>]*>, in the preceding code section, we tried to remove all of the HTML markup from the movie reviews. Although many programmers generally advise against the use of regex to parse HTML, this regex should be sufficient to clean this particular dataset. Since we are only interested in removing HTML markup and do not plan to use the HTML markup further, using regex to do the job should be acceptable. However, if you prefer to use sophisticated tools for removing HTML markup from text, you can take a look at Pythonâ€™s HTML parser module, which is described at https://docs.python.org/3/library/html.parser.html. After we removed the HTML markup, we used a slightly more complex regex to find emoticons, which we temporarily stored as emoticons. Next, we removed all non-word characters from the text via the regex [\\W]+ and converted the text into lowercase characters."
      ],
      "metadata": {
        "id": "BSkxW_3ku4kr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop-words"
      ],
      "metadata": {
        "id": "0QxyZSwIwOUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most common words that may not contribute much information are called stop words. We may consider removing these when pre-processing the text."
      ],
      "metadata": {
        "id": "8nGSSvUmxYMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "DSCSHHQzwTHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "[w for w in tokenizer_porter('a runner likes running and runs a lot') if w not in stop]\n",
        "['runner', 'like', 'run', 'run', 'lot']"
      ],
      "metadata": {
        "id": "gkNi1jUKwciZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "', '.join(sorted(stop))"
      ],
      "metadata": {
        "id": "7GccDf1OwvEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# n-grams"
      ],
      "metadata": {
        "id": "2O7HcPuRx0IC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can make tokens out of multiple words. This allows us to capture features like \"very bad\" or \"very good\"."
      ],
      "metadata": {
        "id": "dIcUMUHrxzOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count = CountVectorizer(stop_words=None, ngram_range=(1, 2))\n",
        "bag = count.fit_transform(df[\"review\"])"
      ],
      "metadata": {
        "id": "AfalFKIbeEfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pprint\n",
        "pprint.pprint(count.vocabulary_)"
      ],
      "metadata": {
        "id": "uUDXhKELfstg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
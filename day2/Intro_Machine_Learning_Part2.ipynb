{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "g0e9vwetcSSc",
        "AWYEQpbTlsz6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m93quVWjYEC3"
      },
      "outputs": [],
      "source": [
        "###\n",
        "#\n",
        "# Load necessary packages\n",
        "#\n",
        "###\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib import rc\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "rc('animation', html='jshtml')\n",
        "\n",
        "\n",
        "###\n",
        "#\n",
        "# Set the random seed\n",
        "#\n",
        "###\n",
        "\n",
        "rng = np.random.RandomState(1)\n",
        "\n",
        "###\n",
        "#\n",
        "# Set colors for plotting\n",
        "#\n",
        "###\n",
        "\n",
        "# four color-blind friendly qualitative colors, and black\n",
        "qualitative_colors = ['#1b9e77','#d95f02','#7570b3','#e7298a']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regression"
      ],
      "metadata": {
        "id": "YTHyH5eFeJAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Some Example Data\n",
        "\n"
      ],
      "metadata": {
        "id": "RlsfScOT4_4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes"
      ],
      "metadata": {
        "id": "l08xjoTfYViZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Make the data set.\n",
        "#\n",
        "###\n",
        "\n",
        "diabetes_bunch = load_diabetes()\n",
        "\n",
        "diabetes_bunch.frame\n",
        "\n",
        "diabetes_X = diabetes_bunch.data\n",
        "diabetes_y = diabetes_bunch.target\n",
        "\n",
        "# Use only one feature\n",
        "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
        "\n",
        "# Split the data into training/testing sets\n",
        "diabetes_X_train = diabetes_X[:-20]\n",
        "diabetes_X_test = diabetes_X[-20:]\n",
        "\n",
        "# Split the targets into training/testing sets\n",
        "diabetes_y_train = diabetes_y[:-20]\n",
        "diabetes_y_test = diabetes_y[-20:]\n",
        "\n",
        "print(diabetes_X_train.shape, diabetes_X_test.shape)\n",
        "print(diabetes_y_train.shape, diabetes_y_test.shape)"
      ],
      "metadata": {
        "id": "tRPwZfz8YXTT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Plot the data set.\n",
        "#\n",
        "###\n",
        "\n",
        "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200)\n",
        "axs.scatter(diabetes_X_train, diabetes_y_train, color=qualitative_colors[0], s=10)\n",
        "axs.set_xlabel('BMI (scaled)')\n",
        "axs.set_ylabel('quantitative measure of diabetes progression')"
      ],
      "metadata": {
        "id": "c34XfB1dYeOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "jGKO7-SeYogp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### A straight line with input $x$ and output $y$ has the form $y = w_1 x + w_0$, where $w_0$ and $w_1$ are coefficients we aim to learn. \n",
        "\n",
        "#### We use the letter $w$ because we think of the coefficients as **weights**; the value of $y$ is changed by changing the relative weight of one term or another.\n",
        "\n",
        "#### The task of finding the function that best fits the training set of $n$ points in the $x$, $y$ plane is called **linear regression**."
      ],
      "metadata": {
        "id": "g0e9vwetcSSc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### To fit the line to the data, all we have to do is find the values of the weights ($w_0 , w_1$) that minimize the *loss*.\n",
        "\n",
        "#### One way is to use the squared-error loss function:\n",
        "\n",
        "## $ \\mathrm{Loss} = \\sum_{j=1}^{n} (y_j - (w_1 x_j + w_0))^2$ . \n"
      ],
      "metadata": {
        "id": "kwO51_HPZ9wg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def squarederror_loss(xs, ys, w0, w1):\n",
        "  \"\"\"Caculate the squared-error loss\n",
        "    Parameters\n",
        "    ----------\n",
        "    xs : array_like\n",
        "        x-axis values of data points, shape (number of data points)\n",
        "    ys : array_like\n",
        "        y-axis values of data points, shape (number of data points)\n",
        "    w0 : array_like\n",
        "        weight for intercept, shape (number of weights)\n",
        "    w1 : array_like\n",
        "        weight for slope, shape (number of weights)\n",
        "    Returns\n",
        "    -----------\n",
        "    loss : array_like\n",
        "        squared-error loss, shape (number of weights)\n",
        "    \"\"\"\n",
        "  xs = np.asarray(xs).flatten()\n",
        "  ys = np.asarray(ys).flatten()\n",
        "  loss = np.sum((ys[:,np.newaxis] - (w1[np.newaxis,:]*xs[:,np.newaxis] + w0[np.newaxis,:]))**2.,axis=0) \n",
        "  return loss"
      ],
      "metadata": {
        "id": "AvvTnsYVZ_Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The squared-error loss function is minimized when the partial derivates with respect to $ w_0 $ and $w_1 $ are zero:\n",
        "\n",
        "## $ \\frac{\\partial}{\\partial w_0} \\sum_{j=1}^{n} (y_j - (w_1 x_j + w_0))^2 = 0 $, \n",
        "\n",
        "## $ \\frac{\\partial}{\\partial w_1} \\sum_{j=1}^{n} (y_j - (w_1 x_j + w_0))^2 = 0 $ .\n",
        "\n",
        "#### This has unique soltions:\n",
        "\n",
        "## $ w_0 = \\frac{\\sum y_j - w_1 (\\sum x_j) }{N} $\n",
        "\n",
        "## $ w_1 = \\frac{N (\\sum x_j y_j) - (\\sum x_j)(\\sum y_j)}{N(\\sum x^2_j) - (\\sum x_j)^2} $"
      ],
      "metadata": {
        "id": "E6RrHTrKZ_3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def univariate_linear_regression(xs, ys):\n",
        "  \"\"\"Calculate optimal weights in the univariate linear regression case.\n",
        "    Parameters\n",
        "    ----------\n",
        "    xs : array_like\n",
        "        x-axis values of data points, shape (number of data points)\n",
        "    ys : array_like\n",
        "        y-axis values of data points, shape (number of data points)\n",
        "    Returns\n",
        "    -----------\n",
        "    w0 : float\n",
        "        weight for intercept\n",
        "    w1 : float\n",
        "        weight for slope\n",
        "    \"\"\"\n",
        "  xs = np.asarray(xs).flatten()\n",
        "  ys = np.asarray(ys).flatten()\n",
        "  N = float(xs.shape[0])\n",
        "  w1 = (N*np.sum(xs*ys) - np.sum(xs)*np.sum(ys) ) / ( N*np.sum(xs**2.) - np.sum(xs)**2.)\n",
        "  w0 = (np.sum(ys) - w1*np.sum(xs)) / N\n",
        "  return w0, w1"
      ],
      "metadata": {
        "id": "tv4OHTXcbcB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w0, w1 = univariate_linear_regression(diabetes_X_train, diabetes_y_train)\n",
        "print(w0, w1)"
      ],
      "metadata": {
        "id": "ZfgPXggTecbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Plot the data set with the best fit.\n",
        "#\n",
        "###\n",
        "\n",
        "plot_xs = np.linspace(diabetes_X_train.min(), diabetes_X_train.max(), 101)\n",
        "\n",
        "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200)\n",
        "axs.scatter(diabetes_X_train, diabetes_y_train, s=1, color=qualitative_colors[0])\n",
        "axs.plot(plot_xs, w1*plot_xs + w0, color=qualitative_colors[1], linewidth=2)\n",
        "axs.set_xlabel('BMI (scaled)')\n",
        "axs.set_ylabel('quantitative measure of diabetes progression')"
      ],
      "metadata": {
        "id": "xeEzVs7AelWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We don't have to just use this optimal weight value, we can explore the weight space.\n",
        "\n",
        "#### We'll use the *squarederror_loss* function we defined earlier to calcule the loss over a wide range of $w_0$ and $w1$ values."
      ],
      "metadata": {
        "id": "sZRNB12c7zlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Define a range of w0 and w1 values to calculate.\n",
        "#\n",
        "###\n",
        "\n",
        "w0_min = w0-20.\n",
        "w0_max = w0+20.\n",
        "w1_min = w1-20.\n",
        "w1_max = w1+20.\n",
        "\n",
        "###\n",
        "#\n",
        "# Make a 200 x 200 grids of w0 and w1 values over that range.\n",
        "#\n",
        "###\n",
        "\n",
        "XX, YY = np.mgrid[w0_min:w0_max:200j, w1_min:w1_max:200j]\n",
        "\n",
        "###\n",
        "#\n",
        "# Flatten the grids to lists with 40000 values.\n",
        "#\n",
        "###\n",
        "\n",
        "temp = np.c_[XX.ravel(), YY.ravel()]"
      ],
      "metadata": {
        "id": "VGvaHZIzfJJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Calculate the squared-error loss for each pair of w0, w1 values.\n",
        "#\n",
        "###\n",
        "\n",
        "Z = squarederror_loss(diabetes_X_train, diabetes_y_train, temp[:,0], temp[:,1])\n",
        "\n",
        "###\n",
        "#\n",
        "# Turn the list of loss values into a 200 x 200 grid.\n",
        "#\n",
        "###\n",
        "Z = Z.reshape(XX.shape)\n",
        "\n",
        "###\n",
        "#\n",
        "# Also calculate the loss for the optimal w0, w1 values we found earlier.\n",
        "#\n",
        "###\n",
        "best_fit_loss = squarederror_loss(diabetes_X_train, diabetes_y_train, np.array([w0]), np.array([w1]))"
      ],
      "metadata": {
        "id": "3iUDRt0Ghjej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Plot the squared-error loss values.\n",
        "#\n",
        "###\n",
        "\n",
        "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200, sharex=True)\n",
        "CS = axs.contour(XX, YY, Z)\n",
        "axs.clabel(CS, inline=True, fontsize=10, fmt='%d')\n",
        "axs.scatter(w0, w1, s=10, color='k')\n",
        "axs.set_ylabel(r'$w_1$')\n",
        "axs.set_xlabel(r'$w_0$')\n",
        "\n",
        "print(best_fit_loss)"
      ],
      "metadata": {
        "id": "nDbg5iU1i2AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This is an ideal case, where it is easy to find an optimal solution where the partial derivates are zero.\n",
        "\n",
        "#### What are other methods for minimizing loss that does not depend  on solving partial derivates and can be applied to any loss function?\n",
        "\n",
        "#### We can search through a continuous weight space looking for the minimum, using a technique called *gradient decent*."
      ],
      "metadata": {
        "id": "AWYEQpbTlsz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Decent"
      ],
      "metadata": {
        "id": "YtdVf8q59bvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0\n",
        "\n",
        "#### Choose a starting point in the weight space - a point in the ($w_0$, $w_1$).\n",
        "#### A simple method to do this is to randomly choose a point, but this could be very far from the optimal position.\n",
        "\n",
        "#### Usually you'll have some intuition about your data and will choose several values close to what you expect and rerun the procedure to see if you get the same answer."
      ],
      "metadata": {
        "id": "lIzpihvv9z-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w0_init, w1_init = w0-10., w1-10."
      ],
      "metadata": {
        "id": "iqjeu9wK-Odi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1\n",
        "\n",
        "#### Compute an estimate of the gradient at this point.\n",
        "\n",
        "#### For the univariate case:\n",
        "\n",
        "## $ \\frac{\\partial}{\\partial w_0} \\sum_{j=1}^{n} (y_j - (w_1 x_j + w_0))^2 = -2 \\sum_{j=1}^{n}(y_j - (w_1 x_j + w_0)) $, \n",
        "\n",
        "## $ \\frac{\\partial}{\\partial w_1} \\sum_{j=1}^{n} (y_j - (w_1 x_j + w_0))^2 = -2 \\sum_{j=1}^{n}(y_j - (w_1 x_j + w_0)) x_j $ .\n"
      ],
      "metadata": {
        "id": "BqVG_K4V-R80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_of_weights(xs, ys, w0, w1):\n",
        "  \"\"\"Calculate the partial derivative of the loss function with respect to the weights.\n",
        "  Parameters\n",
        "  ----------\n",
        "  xs : array_like\n",
        "      x-axis values of data points, shape (number of data points)\n",
        "  ys : array_like\n",
        "      y-axis values of data points, shape (number of data points)\n",
        "  w0 : float\n",
        "    weight for intercept\n",
        "  w1 : float\n",
        "      weight for slope\n",
        "  Returns\n",
        "  -----------\n",
        "  derivative_w0 : float\n",
        "      partial derivative of the loss function with respect to theweight for slope\n",
        "  derivative_w1 : float\n",
        "      partial derivative of the loss function with respect to the weight for slope\n",
        "  \"\"\"\n",
        "\n",
        "  derivative_w0 = np.sum((ys - (w1*xs + w0)) )\n",
        "  derivative_w1 = np.sum((ys - (w1_init*xs + w0_init))*xs)\n",
        "\n",
        "  return derivative_w0, derivative_w1"
      ],
      "metadata": {
        "id": "iE5HndSl_Ofl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2\n",
        "\n",
        "#### Move a small amount from the initial value in the steepest downhill direction.\n",
        "\n",
        "#### We specfic the *small amount* to move as $\\alpha$. This is often called the *learning rate* or *step size*.\n",
        "\n",
        "#### The initial $w_0$ and $w_1$ are updated in the following way:\n",
        "\n",
        "## $ w_0 ← w_0 + \\alpha \\sum_{j=1}^{n}(y_j - (w_1 x_j + w_0)) $, \n",
        "\n",
        "## $ w_1 ← w_1 + \\alpha \\sum_{j=1}^{n}(y_j - (w_1 x_j + w_0)) x_j $ ."
      ],
      "metadata": {
        "id": "8WmB_KN5AlaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_weights(xs, ys, w0_old, w1_old, step_size):\n",
        "  \"\"\" Update the weights using the partial derivate and step size.\n",
        "  Parameters\n",
        "  ----------\n",
        "  xs : array_like\n",
        "      x-axis values of data points, shape (number of data points)\n",
        "  ys : array_like\n",
        "      y-axis values of data points, shape (number of data points)\n",
        "  w0_old : float\n",
        "    weight for intercept\n",
        "  w1_old : float\n",
        "      weight for slope\n",
        "  Returns\n",
        "  -----------\n",
        "  w0_new: float\n",
        "    weight for intercept\n",
        "  w1_new : float\n",
        "    weight for slope\n",
        "  \"\"\"\n",
        "  xs = np.asarray(xs).flatten()\n",
        "  ys = np.asarray(ys).flatten()\n",
        "  w0_new = w0_old + step_size*np.sum((ys - (w1_old*xs + w0_old)) )\n",
        "  w1_new = w1_old + step_size*np.sum((ys - (w1_old*xs + w0_old))*xs)\n",
        "  return w0_new, w1_new"
      ],
      "metadata": {
        "id": "ABHuD3blBlwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeat Steps 1 - 2\n",
        "\n",
        "### until difference between the old and the new weights is sufficiently small."
      ],
      "metadata": {
        "id": "xvorriZFCVMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def univariate_gradient_decent(xs, ys, w0_init, w1_init, step_size = 0.001, sufficiently_small = 0.00001):\n",
        "  \"\"\" Update the weights using the partial derivate and step size.\n",
        "  Parameters\n",
        "  ----------\n",
        "  xs : array_like\n",
        "      x-axis values of data points, shape (number of data points)\n",
        "  ys : array_like\n",
        "      y-axis values of data points, shape (number of data points)\n",
        "  w0_init : float\n",
        "    initial guess of weight for intercept\n",
        "  w1_init : float\n",
        "    initial guess of weight for slope\n",
        "  Returns\n",
        "  -----------\n",
        "  w0s: array_like\n",
        "    list of weights for intercept\n",
        "  w1s : array_like\n",
        "    list of weights for slope\n",
        "  \"\"\"\n",
        "  w0_firststep, w1_firststep = update_weights(xs, ys, w0_init, w1_init, step_size)\n",
        "  \n",
        "  w0s = np.array( [w0_init, w0_firststep] )\n",
        "  w1s = np.array( [w1_init, w1_firststep] )\n",
        "  \n",
        "  while np.any([np.abs(w0s[-2]-w0s[-1]) > sufficiently_small, np.abs(w1s[-2]-w1s[-1]) > sufficiently_small]):\n",
        "    w0_nextstep, w1_nextstep = update_weights(xs, ys, w0s[-1], w1s[-1], step_size)\n",
        "    w0s = np.append(w0s, w0_nextstep)\n",
        "    w1s = np.append(w1s, w1_nextstep)\n",
        "  return w0s, w1s"
      ],
      "metadata": {
        "id": "4-NqtcDFkZ5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w0, w1)\n",
        "print(w0_init, w1_init)\n",
        "print(update_weights(diabetes_X_train, diabetes_y_train, w0_init, w1_init, step_size = 0.001))"
      ],
      "metadata": {
        "id": "g-jrb-dgJWji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights_to_plot0, weights_to_plot1 = univariate_gradient_decent(diabetes_X_train, diabetes_y_train, w0_init, w1_init)"
      ],
      "metadata": {
        "id": "QEvHSjHRrTnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Plot the squared-error loss values.\n",
        "#\n",
        "###\n",
        "\n",
        "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200, sharex=True)\n",
        "CS = axs.contour(XX, YY, Z)\n",
        "axs.clabel(CS, inline=True, fontsize=10, fmt='%d')\n",
        "axs.plot(weights_to_plot0, weights_to_plot1, '-', color='k')\n",
        "axs.scatter(w0, w1, s=10, color='k')\n",
        "axs.set_ylabel(r'$w_1$')\n",
        "axs.set_xlabel(r'$w_0$')\n",
        "axs.set_xlim(w0-20., w0+20.)\n",
        "axs.set_ylim(w1-20., w1+20.)"
      ],
      "metadata": {
        "id": "nUYTyh2arWn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### This is a very simple example, in just two dimensions, but it can extend to many dimensions."
      ],
      "metadata": {
        "id": "sKJbfPQvb4fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the full diabetes dataset\n",
        "diabetes_dataset = load_diabetes(as_frame=True)"
      ],
      "metadata": {
        "id": "2_14ttgycQWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(diabetes_dataset.frame, corner=True)"
      ],
      "metadata": {
        "id": "eBw9q1bpeYob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = diabetes_dataset.data\n",
        "print(features.columns)\n",
        "features = features.drop(columns=['sex'])\n",
        "print(features.columns)"
      ],
      "metadata": {
        "id": "HIjhLfw4G6Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model"
      ],
      "metadata": {
        "id": "isMTsIkPb9uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg = linear_model.LinearRegression()\n",
        "reg.fit(features, diabetes_dataset.target)\n",
        "print(reg.coef_)\n",
        "print(reg.intercept_)"
      ],
      "metadata": {
        "id": "3-mKJgDVcAdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg = linear_model.SGDRegressor(loss='squared_error', max_iter=10000)\n",
        "reg.fit(features, diabetes_dataset.target)\n",
        "print(reg.coef_)\n",
        "print(reg.intercept_)"
      ],
      "metadata": {
        "id": "mtWGy9MqEquf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "gn4WD4IJeCac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Some Example Data"
      ],
      "metadata": {
        "id": "yuGcyzsxHy64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification"
      ],
      "metadata": {
        "id": "wKVh1m2SvWgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Make the data set.\n",
        "#\n",
        "###\n",
        "\n",
        "X, labels = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=0, n_clusters_per_class=1, class_sep=3.)\n",
        "X += 2 * rng.uniform(size=X.shape)"
      ],
      "metadata": {
        "id": "dFQjaUlozXxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Plot the data set.\n",
        "#\n",
        "###\n",
        "\n",
        "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200)\n",
        "axs.scatter(X[:, 0], X[:, 1], marker=\"o\", c=labels, s=25, edgecolor=\"k\")\n",
        "axs.set_xlabel('$x_1$')\n",
        "axs.set_ylabel('$x_2$')"
      ],
      "metadata": {
        "id": "zuSzGYAgzebb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Boundary\n"
      ],
      "metadata": {
        "id": "TNmZUDPcn28L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A *decision boundary* is a line (or a surface in more than two dimensions) that seperates two classes.\n",
        "#### Linearly seperable data can be divided using a linear decision boundary:\n",
        "\n",
        "## $ x_2 - (w_1 x_1 + w_0) = 0 $\n",
        "\n",
        "#### The yellow points, which we want to classify with value 1, are above this line; they are points for which:\n",
        "## $ x_2 - (w_1 x_1 + w_0) > 0 $,\n",
        "#### while the purple points, which we want to classify with value 0, are \n",
        "## $ x_2 - (w_1 x_1 + w_0) < 0 $."
      ],
      "metadata": {
        "id": "jt7g8F9GcXJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Plot the data set and inital decision boundary guess.\n",
        "#\n",
        "###\n",
        "\n",
        "plot_xs = np.linspace(X[:, 0].min(), X[:, 0].max(), 101)\n",
        "\n",
        "w0_init, w1_init = 2., 0.\n",
        "\n",
        "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200)\n",
        "axs.scatter(X[:, 0], X[:, 1], marker=\"o\", c=labels, s=25, edgecolor=\"k\")\n",
        "axs.plot(plot_xs, w1_init*plot_xs + w0_init, color='k', linewidth=2)\n",
        "axs.set_xlabel('$x_1$')\n",
        "axs.set_ylabel('$x_2$')"
      ],
      "metadata": {
        "id": "KZbPdNeIJ9a5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define a function that predicts the class based on the point's position relative to the decision boundary."
      ],
      "metadata": {
        "id": "NUWEyDBpMAZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_class(x1s, x2s, w0, w1):\n",
        "  \"\"\" Predict the class of each point.\n",
        "  Parameters\n",
        "  ----------\n",
        "  x1s : array_like\n",
        "      x-axis values of data points, shape (number of data points)\n",
        "  x2s : array_like\n",
        "      y-axis values of data points, shape (number of data points)\n",
        "  w0 : float\n",
        "    weight for intercept\n",
        "  w1 : float\n",
        "    weight for slope\n",
        "  Returns\n",
        "  -----------\n",
        "  prediction: array_like\n",
        "    list of predicted classes\n",
        "  \"\"\"\n",
        "  prediction = np.asarray((x2s - (w1*x1s + w0)) > 0, dtype=int)\n",
        "  return prediction"
      ],
      "metadata": {
        "id": "h8ktUyEwLCL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict_class(X[:, 0], X[:, 1], w0_init, w1_init)\n",
        "print(predictions)\n",
        "print(predictions == labels)\n",
        "print(np.sum(predictions == labels), labels.shape[0])"
      ],
      "metadata": {
        "id": "-W4sU7HALn9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Update the weights based on the points that miscalssified:"
      ],
      "metadata": {
        "id": "WspX7nK-Megd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_weights_classification(x1s, x2s, w0_old, w1_old, labels, step_size):\n",
        "  \"\"\" Update the weights based on the predicted classes.\n",
        "  Parameters\n",
        "  ----------\n",
        "  x1s : array_like\n",
        "      x-axis values of data points, shape (number of data points)\n",
        "  x2s : array_like\n",
        "      y-axis values of data points, shape (number of data points)\n",
        "  w0_old : float\n",
        "    weight for intercept\n",
        "  w1_old : float\n",
        "    weight for slope\n",
        "  labels : array_like\n",
        "    true classes\n",
        "  step_size : float\n",
        "  Returns\n",
        "  -----------\n",
        "  w0_new: float\n",
        "    weight for intercept\n",
        "  w1_new : float\n",
        "    weight for slope\n",
        "  \"\"\"\n",
        "  predictions = predict_class(x1s, x2s, w0_old, w1_old)\n",
        "  if np.sum(predictions == labels) == labels.shape[0]:\n",
        "    # weights are unchanged\n",
        "    return w0_old, w1_old\n",
        "  else:\n",
        "    if np.sum((labels == 1) & (predictions == 0)) > 0:\n",
        "      # if true class is yellow and predicted to be purple\n",
        "      w0_new = w0_old - step_size # decrease intercept of decision boundary\n",
        "      if np.sum((labels == 1) & (predictions == 0) & (x1s > 0.)) > 0:\n",
        "        # if incorrectly classifed points are on right side\n",
        "        w1_new = w1_old - step_size # decrease slope of decision boundary\n",
        "      else:\n",
        "        # if incorrectly classifed points are on left side\n",
        "        w1_new = w1_old + step_size # increase slope of decision boundary\n",
        "    \n",
        "    if np.sum((labels == 0) & (predictions == 1)) > 0:\n",
        "      # if true class is purple and predicted to be yellow\n",
        "      w0_new = w0_old + step_size # increase intercept of decision boundary\n",
        "      if np.sum((labels == 0) & (predictions == 1) & (x1s > 0.)) > 0:\n",
        "        # if incorrectly classifed points are on right side\n",
        "        w1_new = w1_old + step_size # increase slope of decision boundary\n",
        "      else:\n",
        "        # if incorrectly classifed points are on left side\n",
        "        w1_new = w1_old - step_size # decrease slope of decision boundary\n",
        "  \n",
        "    return w0_new, w1_new"
      ],
      "metadata": {
        "id": "bNpUqbk-JrFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w0_init, w1_init = 4.0, 0.0\n",
        "print(w0_init, w1_init)\n",
        "w0_new, w1_new = update_weights_classification(X[:, 0], X[:, 1], w0_init, w1_init, labels, step_size = 0.001)\n",
        "print(w0_new, w1_new)"
      ],
      "metadata": {
        "id": "OaG7-4jyPWxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similar to regression, we can iteratively update the weights until we have optimized the decision boundary:"
      ],
      "metadata": {
        "id": "bo08LyxcRPzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_classification_boundary(x1s, x2s, labels, w0_init, w1_init, step_size = 0.001, sufficiently_small=0.00001):\n",
        "  \"\"\" Update the weights using the partial derivate and step size.\n",
        "  Parameters\n",
        "  ----------\n",
        "  x1s : array_like\n",
        "      x-axis values of data points, shape (number of data points)\n",
        "  x2s : array_like\n",
        "      y-axis values of data points, shape (number of data points)\n",
        "  w0_init : float\n",
        "    initial guess of weight for intercept\n",
        "  w1_init : float\n",
        "    initial guess of weight for slope\n",
        "  Returns\n",
        "  -----------\n",
        "  w0s: array_like\n",
        "    list of weights for intercept\n",
        "  w1s : array_like\n",
        "    list of weights for slope\n",
        "  \"\"\"\n",
        "  w0_firststep, w1_firststep = update_weights_classification(x1s, x2s, w0_init, w1_init, labels, step_size)\n",
        "  \n",
        "  w0s = np.array( [w0_init, w0_firststep] )\n",
        "  w1s = np.array( [w1_init, w1_firststep] )\n",
        "  \n",
        "  while np.any([np.abs(w0s[-2]-w0s[-1]) > sufficiently_small, np.abs(w1s[-2]-w1s[-1]) > sufficiently_small]):\n",
        "    w0_nextstep, w1_nextstep = update_weights_classification(x1s, x2s, w0s[-1], w1s[-1], labels, step_size)\n",
        "    w0s = np.append(w0s, w0_nextstep)\n",
        "    w1s = np.append(w1s, w1_nextstep)\n",
        "  return w0s, w1s"
      ],
      "metadata": {
        "id": "qbSP0v3WnGAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w0s, w1s = optimize_classification_boundary(X[:, 0], X[:, 1], labels, 4.0, 0.0)"
      ],
      "metadata": {
        "id": "bkA1UccLpWzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Plot the data set, inital decision boundary guess, and final decision boundary.\n",
        "#\n",
        "###\n",
        "\n",
        "plot_xs = np.linspace(X[:, 0].min(), X[:, 0].max(), 101)\n",
        "\n",
        "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200)\n",
        "axs.scatter(X[:, 0], X[:, 1], marker=\"o\", c=labels, s=25, edgecolor=\"k\")\n",
        "axs.plot(plot_xs, w1s[0]*plot_xs + w0s[0], color='k', linewidth=2)\n",
        "axs.plot(plot_xs, w1s[-1]*plot_xs + w0s[-1], color=qualitative_colors[1], linewidth=2)\n",
        "axs.set_xlabel('$x_1$')\n",
        "axs.set_ylabel('$x_2$')"
      ],
      "metadata": {
        "id": "nMJFhYhXrejv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification with Sci-Kit Learn"
      ],
      "metadata": {
        "id": "FqyowdxHq79m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier"
      ],
      "metadata": {
        "id": "W9NP1D4orGz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = SGDClassifier(loss='log', penalty='l2', alpha=0.001, max_iter=100)\n",
        "clf.fit(X, labels)\n",
        "print(clf.coef_[0])\n",
        "print(clf.intercept_)"
      ],
      "metadata": {
        "id": "gVV1WiI_tDb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200)\n",
        "axs.scatter(X[:, 0], X[:, 1], marker=\"o\", c=labels, s=25, edgecolor=\"k\")\n",
        "axs.plot(plot_xs, (-(plot_xs * clf.coef_[0, 0]) - clf.intercept_[0]) / clf.coef_[0, 1], color='k', linewidth=2)\n",
        "axs.set_xlabel('$x_1$')\n",
        "axs.set_ylabel('$x_2$')"
      ],
      "metadata": {
        "id": "uEw_Z0z0rK6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clustering"
      ],
      "metadata": {
        "id": "CuGubmA9xh7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Some Example Data"
      ],
      "metadata": {
        "id": "WpDsEsqGceJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs"
      ],
      "metadata": {
        "id": "wJ1rrDjUxlm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Make the data set.\n",
        "#\n",
        "###\n",
        "\n",
        "\n",
        "number_of_data_points = 500\n",
        "\n",
        "X, y = make_blobs(n_samples=number_of_data_points, random_state=1)"
      ],
      "metadata": {
        "id": "tTvdihtfrR_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Plot the data set.\n",
        "#\n",
        "###\n",
        "fig, axs = plt.subplots(figsize=(4., 4.), nrows=1, ncols=1, facecolor='white', dpi=200)  # create an empty figure\n",
        "axs.plot(X[:, 0], X[:, 1], marker='o', linewidth=0.0, markerfacecolor='k', markeredgecolor='none', markersize=4, zorder=0)  # add the data points\n",
        "axs.set_xlabel(r'$x_1$')  # label the axes\n",
        "axs.set_ylabel(r'$x_2$');"
      ],
      "metadata": {
        "id": "p_bMaUdXxkB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##K-means Algorithm\n"
      ],
      "metadata": {
        "id": "Iv8ZAvwzxvAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 0\n",
        "\n",
        "#### Choose the initial centers of the clusters.\n",
        "#### A simple method to do this is to randomly choose samples points from the dataset."
      ],
      "metadata": {
        "id": "O9Xrb4J4xy3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_clusters = 3\n",
        "\n",
        "###\n",
        "#\n",
        "# Randomly choose K points from the data set, without replacement.\n",
        "#\n",
        "###\n",
        "rng = np.random.default_rng(seed=21474836)\n",
        "centroids_initial = rng.choice(X, size=number_of_clusters, replace=False, axis=0)\n",
        "print(centroids_initial.shape)"
      ],
      "metadata": {
        "id": "XXtlQFOoxrMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Plot the data set and the initial centers of the clusters.\n",
        "#\n",
        "###\n",
        "fig, axs = plt.subplots(figsize=(4., 4.), nrows=1, ncols=1, facecolor='white', dpi=200)\n",
        "axs.plot(X[:, 0], X[:, 1], marker='o', linewidth=0.0, markerfacecolor='k', markeredgecolor='none', markersize=4, zorder=-10)\n",
        "for j in range(number_of_clusters):\n",
        "    axs.plot(centroids_initial[j, 0], centroids_initial[j, 1], marker='D', markerfacecolor=qualitative_colors[j], markeredgecolor='k', zorder=0)\n",
        "axs.set_xlabel(r'$x_1$')\n",
        "axs.set_ylabel(r'$x_2$');"
      ],
      "metadata": {
        "id": "e1hwUXF1x3J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1\n",
        "\n",
        "#### Assign each data point to a cluster based on which cluster center is closest."
      ],
      "metadata": {
        "id": "Ej7XMNWYybJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Define a function to perform the first step.\n",
        "#\n",
        "###\n",
        "def calculate_nearest_cluster_center(X, centroids):\n",
        "    \"\"\"Assign each data point to a cluster based on which cluster center is closest.\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        data points, shape (number of data points, number of features)\n",
        "    centroids : array_like\n",
        "        centers of each cluster, shape (number of clusters, number of features)\n",
        "    Returns\n",
        "    -----------\n",
        "    nearest_cluster_center : array_like\n",
        "        cluster each data point is assigned to, shape (number of data points, 1)\n",
        "    \"\"\"\n",
        "    n_data_points = X.shape[0]\n",
        "    n_clusters = centroids.shape[0]\n",
        "    distances = np.empty((n_data_points, n_clusters))  # create an empty array to place the distances\n",
        "    for i in range(n_clusters):  # iterate over clusters\n",
        "        distances[:, i] = np.sqrt(np.power(X[:, 0] - centroids[i, 0], 2) + np.power(X[:, 1] - centroids[i, 1],2))  # calculate distance between data points and cluster center\n",
        "    nearest_cluster_center = np.argmin(distances, axis=1)  # return cluster center with minimum distance for each data point\n",
        "    return nearest_cluster_center"
      ],
      "metadata": {
        "id": "ClK6kB0lyJER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_cluster = calculate_nearest_cluster_center(X, centroids_initial)"
      ],
      "metadata": {
        "id": "DygonN55yeb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Plot the data set colored based on the cluster assignments and the initial centers of the clusters.\n",
        "#\n",
        "###\n",
        "fig, axs = plt.subplots(figsize=(4., 4.), nrows=1, ncols=1, facecolor='white', dpi=200)\n",
        "for j in range(number_of_clusters):\n",
        "    axs.plot(X[predicted_cluster == j, 0], X[predicted_cluster == j, 1], marker='o', linewidth=0.0,\n",
        "             markerfacecolor=qualitative_colors[j], markeredgecolor='none', markersize=4, zorder=-10)\n",
        "    axs.plot(centroids_initial[j, 0], centroids_initial[j, 1], marker='D', markerfacecolor=qualitative_colors[j],\n",
        "             markeredgecolor='k', zorder=0)\n",
        "axs.set_xlabel(r'$x_1$')\n",
        "axs.set_ylabel(r'$x_2$');"
      ],
      "metadata": {
        "id": "ALPgIjEKygrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2\n",
        "\n",
        "#### Determine new cluster centers by calculating the mean position of all the data points assigned to the cluster."
      ],
      "metadata": {
        "id": "J3AnnV5GyjW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Define a function to perform the second step.\n",
        "#\n",
        "###\n",
        "def calculate_cluster_mean(X, cluster_assignments):\n",
        "    \"\"\"Determine new cluster centers by calculating the mean position of all the data points assigned to the cluster.\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : array_like\n",
        "        data points, shape (number of data points, number of features)\n",
        "    cluster_assignments : array_like\n",
        "        cluster each data point is assigned to, shape (number of data points, 1)\n",
        "    Returns\n",
        "    -----------\n",
        "    mean_positions : array_like\n",
        "        mean position of all the data points assigned to the cluster, shape (number of clusters, number of features)\n",
        "    \"\"\"\n",
        "    n_clusters = np.unique(cluster_assignments).shape[0]\n",
        "    n_features = X.shape[1]\n",
        "    mean_positions = np.empty((n_clusters, n_features))  # make an empty array to place the mean positions in\n",
        "    for i in range(n_clusters):  # iterate over clusters\n",
        "        mean_positions[i, :] = np.mean(X[cluster_assignments == i, :], axis=0)  # calculate the mean position of the data points along each axis\n",
        "    return mean_positions"
      ],
      "metadata": {
        "id": "e-j8FTeHyivC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centroids_new = calculate_cluster_mean(X, predicted_cluster)"
      ],
      "metadata": {
        "id": "i3vNkrKhypOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Plot the data set colored based on the cluster assignments, the new centers of the clusters, and how the cluster centers changed.\n",
        "#\n",
        "###\n",
        "fig, axs = plt.subplots(figsize=(4., 4.), nrows=1, ncols=1, facecolor='white', dpi=200)\n",
        "for j in range(number_of_clusters):\n",
        "    axs.plot(X[predicted_cluster == j, 0], X[predicted_cluster == j, 1], marker='o', linewidth=0.0,\n",
        "             markerfacecolor=qualitative_colors[j], markeredgecolor='none', markersize=4, zorder=-10)\n",
        "    axs.plot([centroids_initial[j, 0], centroids_new[j, 0]], [centroids_initial[j, 1], centroids_new[j, 1]], ls='-',\n",
        "             color='k', zorder=0)\n",
        "    axs.plot(centroids_new[j, 0], centroids_new[j, 1], marker='D', markerfacecolor=qualitative_colors[j],\n",
        "             markeredgecolor='k', zorder=0)\n",
        "axs.set_xlabel(r'$x_1$')\n",
        "axs.set_ylabel(r'$x_2$');"
      ],
      "metadata": {
        "id": "tHQU09Hjyqne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3\n",
        "\n",
        "#### Compute the distance between the old and the new cluster centers."
      ],
      "metadata": {
        "id": "iFUHeyqyyvFP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "#\n",
        "# Define a function to perform the third step.\n",
        "#\n",
        "###\n",
        "def calculate_difference_centers(c_new, c_old):\n",
        "    \"\"\"Calculate the distance between the old and the new cluster centers.\n",
        "    Parameters\n",
        "    ----------\n",
        "    c_new : array_like\n",
        "        current centers of each cluster, shape (number of clusters, number of features)\n",
        "    c_old : array_like\n",
        "        previous centers of each cluster, shape (number of clusters, number of features)\n",
        "    Returns\n",
        "    -----------\n",
        "    mean_positions : array_like\n",
        "        distance between the old and the new cluster centers, shape (number of clusters, 1)\n",
        "    \"\"\"\n",
        "\n",
        "    differences = np.sqrt(np.power(c_new[:, 0] - c_old[:, 0], 2) + np.power(c_new[:, 1] - c_old[:, 1],\n",
        "                                                                            2))  # calculate distance between new cluster centers and old cluster centers\n",
        "    return differences"
      ],
      "metadata": {
        "id": "6KmJJUkDysfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "centroid_diff = calculate_difference_centers(centroids_new, centroids_initial)"
      ],
      "metadata": {
        "id": "WXc6qfa5y3ic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repeat Steps 1 - 3 \n",
        "\n",
        "#### until difference between the old and the new centroids is sufficiently small"
      ],
      "metadata": {
        "id": "Iy2V4UKsy1DP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_centroids(num, scatter_points, centroid_points, centroid_line, X):\n",
        "  \n",
        "  if num == 1:\n",
        "    centroids_new = rng.choice(X, size=n_clusters, replace=False, axis=0)\n",
        "  \n",
        "  elif num > 1:\n",
        "    if num % 2 == 0:\n",
        "      centroids = np.empty((n_clusters,2))\n",
        "      for j in range(n_clusters):\n",
        "        centroids[j,:] = centroid_points[j].get_data()\n",
        "      y_predicted = calculate_nearest_cluster_center(X, centroids)\n",
        "    \n",
        "    if num % 2 == 1:\n",
        "      centroids = np.empty((n_clusters,2))\n",
        "      for j in range(n_clusters):\n",
        "        centroids[j,:] = centroid_points[j].get_data()\n",
        "      y_predicted = calculate_nearest_cluster_center(X, centroids)\n",
        "      centroids_new = calculate_cluster_mean(X, y_predicted)\n",
        "      print(calculate_difference_centers(centroids_new, centroids))\n",
        "  \n",
        "  if num > 0 :\n",
        "    for j in range(n_clusters):\n",
        "      if num > 1:\n",
        "        scatter_points[j].set_data(X[y_predicted==j,0], X[y_predicted==j,1])\n",
        "      if num % 2 == 0:\n",
        "          centroid_points[j].set_data(centroids[j,0], centroids[j,1])\n",
        "      if num % 2 == 1:\n",
        "        centroid_points[j].set_data(centroids_new[j,0], centroids_new[j,1])\n",
        "        centroid_line[j].set_data(np.append(centroid_line[j].get_data()[0],centroids_new[j,0]), np.append(centroid_line[j].get_data()[1],centroids_new[j,1]))\n",
        "      \n",
        "  return scatter_points, centroid_points, centroid_line"
      ],
      "metadata": {
        "id": "GBnopI64y0Cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rng = np.random.default_rng(seed=21474836)\n",
        "n_clusters = 3\n",
        "n_features = 2\n",
        "\n",
        "#Equal Sized Blobs\n",
        "#X, y = make_blobs(n_samples=number_of_data_points, n_features=n_features, random_state = 1)\n",
        "\n",
        "#Un-equal Sized Blobs\n",
        "#X, y = make_blobs(n_samples=number_of_data_points, n_features=n_features, cluster_std=[1.0, 2.5, 0.5], random_state = 3)\n",
        "\n",
        "#Asymmetrical Blobs\n",
        "X, y = make_blobs(n_samples=number_of_data_points, n_features=n_features, random_state = 170)\n",
        "X = np.dot(X, [[0.6, -0.6], [-0.4, 0.8]])"
      ],
      "metadata": {
        "id": "C0Dt9aa9zARs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(3.,3.), facecolor='white', dpi=200)\n",
        "axs = fig.add_subplot()\n",
        "\n",
        "n_iterations = 10\n",
        "\n",
        "y_predicted = np.zeros(X.shape[0])\n",
        "\n",
        "scatter_points = [axs.plot(X[y_predicted==j,0], X[y_predicted==j,1], marker='o', markersize=4, markerfacecolor=qualitative_colors[j], linewidth=0.0, markeredgecolor='none', zorder=0)[0] for j in range(n_clusters)]\n",
        "centroid_points = [axs.plot([ ], [ ], marker='D', markeredgecolor='k', markerfacecolor=qualitative_colors[j], zorder=0)[0] for j in range(n_clusters)]\n",
        "centroid_line = [axs.plot([ ], [ ], c='k', linestyle='-', marker='', zorder=0)[0] for j in range(n_clusters)]\n",
        "\n",
        "axs.set_xlabel(r'$x_1$')\n",
        "axs.set_ylabel(r'$x_2$')\n",
        "\n",
        "# Creating the Animation object\n",
        "ani = animation.FuncAnimation(\n",
        "    fig, update_centroids, frames=2*n_iterations, fargs=(scatter_points, centroid_points, centroid_line, X), interval=200)\n",
        "\n",
        "plt.close()\n",
        "ani"
      ],
      "metadata": {
        "id": "ltqwLQbyzSds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "afiQBdSCzU1U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
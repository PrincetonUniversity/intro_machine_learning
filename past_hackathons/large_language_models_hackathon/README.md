# Large Language Models Hackathon

- Date: Jan. 23, 2024
- Session instructor: Jake Snell

This session introduces the basics of language modeling using the transformer architecture. Participants will learn how to download and fine-tune a large language model using the Hugging Face library.

---

_The following is aimed at future instructors of this hackathon._

Some material that may be good to include in future iterations of this hackathon would be:
- Metrics for language modeling (cross-entropy, perplexity)
- The LLM zoo: GPT, Bloom, PaLM, BERT, RoBERTa, Llama, Mistral, Mixtral, Phi, etc.
- Steps of training an LLM (e.g. supervised fine-tuning, RLHF)
- Prompt engineering: what it is and why it is important
- In-context learning
- High-level overview of fine-tuning approaches: LoRA, prefix tuning, prompt tuning, IA3, etc.

For a more hands-on introduction to LLMs, it may be worth considering incorporating aspects of [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/) into the hackathon.

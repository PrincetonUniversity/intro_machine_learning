{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to Machine Learning  \n",
        "**Computer Vision Hackathon  \n",
        "Wintersession  \n",
        "Tuesday, January 24, 2023**"
      ],
      "metadata": {
        "id": "aga1pGnHqFDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook you will create a CNN from scratch to distinguish cats from dogs."
      ],
      "metadata": {
        "id": "UNycfWo7M0TV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# About Your Colab Session"
      ],
      "metadata": {
        "id": "2zeUMxrssifc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Learn about the CPU-cores for your session:"
      ],
      "metadata": {
        "id": "DuQEJ5K4T6mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat /proc/cpuinfo"
      ],
      "metadata": {
        "id": "kmhl7u9GTJdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "num_cores = min(os.cpu_count(), 2)\n",
        "print(num_cores)"
      ],
      "metadata": {
        "id": "pvmd8gdqqadV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see which GPU we are using (probably a Tesla T4):"
      ],
      "metadata": {
        "id": "boWe_CxtT_NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8yR2en5xCqsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "wGufTwtPso3h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH3SrMfHBejx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to use a GPU when one is available:"
      ],
      "metadata": {
        "id": "OOKVq098sZne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(use_cuda)"
      ],
      "metadata": {
        "id": "Nk3pkXNxCF5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\") if use_cuda else torch.device(\"cpu\")\n",
        "\n",
        "train_kwargs = {'batch_size': 64}\n",
        "test_kwargs  = {'batch_size': 1000}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {'num_workers': num_cores, 'pin_memory': True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)"
      ],
      "metadata": {
        "id": "TVoXo2d6CVwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and unpack the data:"
      ],
      "metadata": {
        "id": "oPIynRI6zWut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://tigress-web.princeton.edu/~jdh4/cats_vs_dogs.tar\n",
        "!tar xf cats_vs_dogs.tar"
      ],
      "metadata": {
        "id": "UbRdgWThzTcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    #transforms.Normalize((0.1307,), (0.3081,))\n",
        "]) \n",
        "train_set = datasets.ImageFolder(root=\"./training_set/\", transform=transform)\n",
        "test_set = datasets.ImageFolder(root=\"./test_set/\", transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, **train_kwargs)\n",
        "test_loader  = torch.utils.data.DataLoader(test_set, shuffle=True, **test_kwargs)\n",
        "\n",
        "image_0, label_0 = train_set[0]\n",
        "print(image_0.shape, label_0)"
      ],
      "metadata": {
        "id": "NgIFFS1TJp-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are roughly 4000 cat images and 4000 dog images in the training set. The test set is roughly 1000 images of each. All images have dimensions 128x128. The cat and dogs images are in color so they are composed of three layers (red, green, blue). The MNIST data set was grayscale so only a single layer was needed per image."
      ],
      "metadata": {
        "id": "NO9cdWhN4ouV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open(\"./training_set/dogs/resized-dog.1001.jpg\")\n",
        "print(f\"Image height: {img.height}\") \n",
        "print(f\"Image width: {img.width}\")\n",
        "img"
      ],
      "metadata": {
        "id": "KkXNTCGY28xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open(\"./training_set/cats/resized-cat.1001.jpg\")\n",
        "print(f\"Image height: {img.height}\") \n",
        "print(f\"Image width: {img.width}\")\n",
        "img"
      ],
      "metadata": {
        "id": "vPX6m50p3laY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition and Hackathon Project"
      ],
      "metadata": {
        "id": "NxvDfF0Ps9uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hackathon project is to create a convolutional neural network from scratch and train in such that it gives a sustained accuracy of 75% or higher on the test set. Your network should use at least 3 convolutional layers.\n",
        "\n",
        "You only need to write the Net class. The rest of the notebook does not need to be changed. After writing the Net class, try running notebook. Raise your hand if you have any questions for the instructor. We're happy to give hints as you work through the exercise."
      ],
      "metadata": {
        "id": "SzBNqmv3M7o3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        # CREATE THE LAYERS HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        # DEFINE THE FORWARD PASS HERE\n",
        "        return output"
      ],
      "metadata": {
        "id": "HYNJjPkeB4Rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate the network and move it to the device (which is a GPU when available). Create the optimizer."
      ],
      "metadata": {
        "id": "QVGbz36OuS8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=1.0)"
      ],
      "metadata": {
        "id": "tvkGwJD_JGEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model, input_size=(3, 128, 128))"
      ],
      "metadata": {
        "id": "kTfbe4QKRYLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Test Methods"
      ],
      "metadata": {
        "id": "KYR03y9dvDEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train() # sets the model in training mode (i.e., dropout enabled)\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "_PrPJRlsCCO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval() # sets the model in evaluation mode (i.e., dropout disabled)\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "bns0Q8O-CFVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train for some number of epochs while reporting the accuracy on the test set periodically:"
      ],
      "metadata": {
        "id": "2Zxo7USTvKMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 12\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=0.7)\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "id": "Yl8Lcz1RJCk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XGZrm2RazBTa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
{
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrincetonUniversity/intro_machine_learning/blob/main/day4/transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "zpCkWYWdmgbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrincetonUniversity/intro_machine_learning/blob/main/day4/transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "zpCkWYWdmgbk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformers\n",
        "\n",
        "Gage DeZoort\n",
        "\n",
        "Wintersession 2025\n",
        "\n",
        "*Adapted from a helpful conversation with ChatGPT.*\n"
      ],
      "metadata": {
        "id": "3-poBvrs6mFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Imports"
      ],
      "metadata": {
        "id": "smb8fYeN7ENJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAaLG3PSyc-w",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "!pip install datasets -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F3gjqazyc-x"
      },
      "source": [
        "\n",
        "\n",
        "The goal of this tutorial is to train a sequence-to-sequence\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-jbjvb8yc-y"
      },
      "source": [
        "## 1. The Learning Task\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1yFgIZpyc-y"
      },
      "source": [
        "Given a word or sequence of words, how likely is some subsequent word? This is a fundamental language modeling task: assigning a likelihood probability to a word to follow some input sequence.\n",
        "\n",
        "\n",
        "As an example, let's consider the following input sequence:\n",
        "\n",
        "*I need to take my dog to the vet because he is*\n",
        "\n",
        "What's the next word? *Hungry*? *Healthy*? *Sick*?\n",
        "\n",
        "You get the picture."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Tokenization\n",
        "\n",
        "Machines need to analyze *tokenized* data. Tokens can be words, phrases, characters, etc. They have corresponding `IDs` that are stored in a lookup table.\n",
        "\n",
        "We're going to use a model called *BERT* (Bidirectional Transformers) as our tokenizer. BERT is a transformer model, whose tokenizer splits the input text into words and punctuation, ignoring whitespace. It also splits complicated words into subwords. See below how the string `\"deeeep\"` which does not appear in the English language, is split into three tokens `['dee', '##ee', '##p']`. The latter two tokens are called *subwords*.\n",
        "\n",
        "Google's propriatary WordPiece algorithm is used to build BERT's vocabulary (of subwords) built iteratively from an initial vocab of single character tokens. Frequent character pairs are merged into new subwords until its 30,000 token vocabulary is constructed.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H7xtViWL430Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Choose a pre-trained model tokenizer (e.g., BERT)\n",
        "model_name = \"bert-base-uncased\" # 100M parameters, not case-sensitive\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Example: Tokenizing text\n",
        "text = \"Transformers are a type of deeeep learning model used for NLP tasks. Epehmeral. Anachronism.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "\n",
        "# Converting tokens to IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Token IDs:\", token_ids)\n",
        "\n",
        "# Decoding token IDs back to text\n",
        "decoded_text = tokenizer.decode(token_ids)\n",
        "print(\"Decoded Text:\", decoded_text)"
      ],
      "metadata": {
        "id": "ISRStxoX6VIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Sequence Data\n",
        "\n",
        "\n",
        "To create a coherent learning task, we need to take sequences of tokens and batch them into inputs with corresponding targets. Sequences are batched into uniform-length chunks. For example consider two words written as sequences of tokens:\n",
        "\n",
        "Sequence #1: `[\"run\", \"##ner\"]`\n",
        "\n",
        "Sequence #2: `[\"d\", \"##run\", \"#k\", \"##en\"]`\n",
        "\n",
        "Our model will expect fixed-size sequences at input, say of size `max_length=3`. Sequence #1 is shorter than `max_length`, so we have to *pad* it with some default value. In BERT, this default value is `[PAD]`. Sequence #2, on the other hand, is longer than `max_length`, so we have to *truncate* it."
      ],
      "metadata": {
        "id": "UyfvjiUldZhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding and truncation\n",
        "\n",
        "sequence = tokenizer(text, padding=\"max_length\", truncation=True, max_length=10)\n",
        "print(\"Encoded Sequence:\", sequence)\n",
        "tokenizer.decode(sequence[\"input_ids\"])"
      ],
      "metadata": {
        "id": "tyLbb0tedgjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the `input_IDs` are what the BERT transformer will actually process, the `token_type_ids` are used to demarkate segments (for next-sentence prediction), and the `attention_mask` indicates which tokens are padding (0). Note that BERT's tokenizer has added a few special tokens. `[CLS]` is a classification token marking the start of the sequence, and `[SEP]` is the separater token marking the end."
      ],
      "metadata": {
        "id": "C1Wh7pHz6aIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Transformer Models\n",
        "\n",
        "BERT is a pre-trained transformer model available for generic use cases. It takes as input the `sequence` data type we generated above and outputs embeddings for each token."
      ],
      "metadata": {
        "id": "dEo-0Scnk89c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Section 4: Understanding Attention ---\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "\n",
        "# Load a pre-trained model\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "# Example input\n",
        "inputs = tokenizer(\"The quick brown fox jumps over the lazy dog.\", return_tensors=\"pt\")\n",
        "\n",
        "# Forward pass through the model\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# The model outputs embeddings\n",
        "print(\"Last hidden state shape:\", outputs.last_hidden_state.shape)"
      ],
      "metadata": {
        "id": "f8_30kqjgQp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we see that each of the 12 words gets a 768 dimensional output embedding. This is a high dimension, so we'll have to use some specialized tools to get a closer look."
      ],
      "metadata": {
        "id": "tWGWW7r0nh2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Attention is All You Need\n",
        "\n",
        "Transformers use attention modules, which quantify how much tokens in a sequence focus on other tokens. Let's take a closer look at how attention works."
      ],
      "metadata": {
        "id": "T2PfMFYNn2vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "\n",
        "# Input text\n",
        "text = \"Transformers are powerful and versatile models.\"\n",
        "\n",
        "# Tokenize and extract embeddings\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "outputs = model(**inputs, output_attentions=True)\n",
        "\n",
        "# Extract hidden states (last layer embeddings)\n",
        "token_embeddings = outputs.last_hidden_state.squeeze(0)  # Shape: [sequence_length, hidden_size]\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "id": "PICExuxUnXNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the embeddings have such a high dimension, we need to use a dimensionality reduction technique called principle component analysis (PCA) to visualize them. PCA identifies mutually-orthogonal directions ($< 768$ of them!) of large variance in the data, returning the projection in this new, lower-dimensional basis."
      ],
      "metadata": {
        "id": "7aqxG0IUpol_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Apply PCA to reduce dimensions to 2D\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(token_embeddings.detach().numpy())\n",
        "\n",
        "# Visualize the reduced embeddings\n",
        "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "plt.figure(figsize=(10, 7))\n",
        "for i, token in enumerate(tokens):\n",
        "    plt.scatter(reduced_embeddings[i, 0], reduced_embeddings[i, 1])\n",
        "    plt.text(reduced_embeddings[i, 0] + 0.01, reduced_embeddings[i, 1] + 0.01, token, fontsize=12)\n",
        "plt.title(\"2D Visualization of Token Embeddings\")\n",
        "plt.xlabel(\"PCA Dimension 1\")\n",
        "plt.ylabel(\"PCA Dimension 2\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dr-e_2ugoEQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the tokens `[\"are\",\"versatile\", \"and\", \"powerful\", \"models\"]` all have very similar embeddings. The sentence start and end tokens, in addition to `\"transformers\"` and the punctuation \".\" are embedded elsewhere."
      ],
      "metadata": {
        "id": "v0_URai2rWpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract attention weights\n",
        "attention_weights = outputs.attentions  # Shape: [num_layers, batch_size, num_heads, seq_len, seq_len]\n",
        "\n",
        "# Example: Visualize attention from the last layer, head 0\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "attention_last_layer = torch.mean(outputs.attentions[-1][0], dim=0).detach().numpy()  # Shape: [seq_len, seq_len]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(attention_last_layer, annot=True, fmt=\".2f\", xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\")\n",
        "plt.title(\"Attention Weights for the Last Layer, Head 0\")\n",
        "plt.xlabel(\"Key Tokens\")\n",
        "plt.ylabel(\"Query Tokens\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f3gwlb-PorKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may notice that [CLS] and [SEP] get the strongest attention weights. `[CLS]` is typically sent to a downstream classification module to analyze the sentiment/meaning of the sequence provided. It may also be used to compare two sequences, e.g. via cosine similarity. [SEP] is usually used in sentence pair analysis; e.g. it can store information about how different two sentences are."
      ],
      "metadata": {
        "id": "YGbqIQhx4q8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregate attention across heads for multiple layers\n",
        "for layer_idx in range(11):\n",
        "    layer_attention = torch.mean(outputs.attentions[layer_idx][0], dim=0).detach().numpy()\n",
        "    sns.heatmap(layer_attention, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\")\n",
        "    plt.title(f\"Layer {layer_idx + 1} Attention (Averaged Across Heads)\")\n",
        "    plt.xlabel(\"Key Tokens\")\n",
        "    plt.ylabel(\"Query Tokens\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "73eJ4QG_4E8H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Sentence Similarity\n",
        "\n",
        "Let's drill down on the embedding stored in `[CLS]` by evaluating several sentences that have (potentially) similar semantic structure."
      ],
      "metadata": {
        "id": "LqRX_zlG5ivt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import CosineSimilarity\n",
        "\n",
        "s1 = \"Transformers are powerful and versatile models.\"\n",
        "s2 = \"Language models like transformers have diverse applications.\"\n",
        "s3 = \"Political polarization keeps us divided and blind to issues that really matter.\"\n",
        "\n",
        "# Tokenize and extract embeddings\n",
        "cls = []\n",
        "for s in [s1, s2, s3]:\n",
        "  inputs = tokenizer(s, return_tensors=\"pt\")\n",
        "  outputs = model(**inputs, output_attentions=True)\n",
        "  token_embeddings = outputs.last_hidden_state.squeeze(0)  # Shape: [sequence_length, hidden_size]\n",
        "  cls.append(token_embeddings[0])\n",
        "\n",
        "# Cosine similarity of each sentence\n",
        "cos_sim = CosineSimilarity(dim=-1)\n",
        "for i in range(3):\n",
        "  for j in range(3):\n",
        "    print(i, j, cos_sim(cls[i], cls[j]))"
      ],
      "metadata": {
        "id": "LpOWFf5k4M7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fine-tuning\n",
        "\n",
        "We've got pre-trained models like BERT available to us. These models have been trained on massive corpora and have excellent general language capabilities. Fine tuning is the process of tuning a pre-trained model, which is a much more efficient approach than re-tuning a language model from scratch."
      ],
      "metadata": {
        "id": "R0KaKCMv8qFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "!pip install evaluate\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "rvg_2g-i6u5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to spin up a smaller version of BERT to fine tune."
      ],
      "metadata": {
        "id": "_B_Bcep2BRzr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "model_name = \"distilbert-base-uncased\" # \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Binary classification"
      ],
      "metadata": {
        "id": "kQB6L3x36hK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [IMDb dataset](https://huggingface.co/datasets/stanfordnlp/imdb) contains 50k movie reviews formatted as input sequences for downstream sentiment analysis. For example, what (0 or 1) do you think the training label would be for this review?\n",
        "\n",
        "*National Treasure is about as over-rated and over-hyped as they come. Nicholas Cage is in no way a believable action hero, and this film is no \"Indiana Jones\". People who have compared this movie to the Indian Jones classic trilogy have seriously fallen off their rocker...*"
      ],
      "metadata": {
        "id": "WtG6oU3XBV2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load IMDb dataset\n",
        "dataset = load_dataset(\"imdb\")"
      ],
      "metadata": {
        "id": "-S3P8SUF9F0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a small fraction of the dataset (e.g., 10%)\n",
        "fraction = 0.1\n",
        "small_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(int(len(dataset[\"train\"]) * fraction)))\n",
        "small_test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(int(len(dataset[\"test\"]) * fraction)))\n",
        "\n",
        "# Verify the size\n",
        "print(f\"Train size: {len(small_train_dataset)}, Test size: {len(small_test_dataset)}\")\n",
        "\n",
        "# Tokenize the smaller datasets\n",
        "def preprocess_data(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "small_train_dataset = small_train_dataset.map(preprocess_data, batched=True)\n",
        "small_test_dataset = small_test_dataset.map(preprocess_data, batched=True)\n",
        "\n",
        "# Convert to PyTorch format\n",
        "small_train_dataset = small_train_dataset.rename_column(\"label\", \"labels\")\n",
        "small_test_dataset = small_test_dataset.rename_column(\"label\", \"labels\")\n",
        "\n",
        "small_train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "small_test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
      ],
      "metadata": {
        "id": "uIRHqsw1_s37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Function to move tensors to the correct device (GPU/CPU)\n",
        "def move_to_device(batch):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    # Move tensor columns to the correct device\n",
        "    batch = {key: value.to(device) if torch.is_tensor(value) else value for key, value in batch.items()}\n",
        "    return batch\n",
        "\n",
        "# Apply this function to your dataset using `map`\n",
        "small_train_dataset = small_train_dataset.map(move_to_device, batched=True)\n",
        "small_test_dataset = small_test_dataset.map(move_to_device, batched=True)"
      ],
      "metadata": {
        "id": "jP-HyFoHIi7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Model device:\", next(model.parameters()).device)  # This should print \"cuda\" if using GPU"
      ],
      "metadata": {
        "id": "BTg7qVj4IvCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    preds = predictions.argmax(axis=-1)  # Get the class with the highest probability\n",
        "    return accuracy.compute(predictions=preds, references=labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=10,\n",
        "    logging_steps=10,\n",
        "    fp16=torch.cuda.is_available(),  # Enable mixed precision if on GPU\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "ycHfWO3L9GIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate()\n",
        "print(results)"
      ],
      "metadata": {
        "id": "TyH_4UjF9O4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9mmHNaZOKD5O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

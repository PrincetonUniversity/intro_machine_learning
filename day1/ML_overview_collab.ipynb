{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrincetonUniversity/intro_machine_learning/blob/main/day1/ML_overview_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_LQ4w2gmfEB"
      },
      "outputs": [],
      "source": [
        "# import libraries used during this workshop\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66qiL7xKmfED"
      },
      "source": [
        "# Data collected from red or white wines\n",
        "\n",
        "Our wine data has the following variables:\n",
        "\n",
        "----\n",
        "\n",
        "1. **fixed acidity**: Most acids in wine are \"fixed\" or nonvolatile (do not evaporate readily).\n",
        "2. **volatile acidity**: The amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste.\n",
        "3. **citric acid**: Found in small quantities, citric acid can add 'freshness' and flavor to wines.\n",
        "4. **residual sugar**: The amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter.\n",
        "5. **chlorides**: The amount of salt in the wine.\n",
        "6. **free sulfur dioxide**: The free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents.\n",
        "7. **quality**: based on sensory data, score between 0 and 10.\n",
        "8. **density**: The density of water is close to that of water depending on the percent alcohol and sugar content.\n",
        "9. **pH**: Describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4.\n",
        "10. **sulphates**: A wine preservative which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial.\n",
        "11. **alcohol**: The percent alcohol content of the wine.\n",
        "12. **total sulfur dioxide**: Amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2.\n",
        "\n",
        "----\n",
        "\n",
        "### Which variable should be our ***response***, with the rest being ***features***? Sometimes it's not obvious and depends on your interests!\n",
        "\n",
        "Load data into a Pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) using the `read_csv()` function, which can read a table on your computer are directly from a URL!."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hs4Yp0k5mfEF"
      },
      "outputs": [],
      "source": [
        "url_red = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
        "url_white = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
        "\n",
        "# these data are in text tables in which the separation of columns is indicated with a semicolon \";\" \n",
        "red_df = pd.read_csv(url_red, sep=\";\")\n",
        "white_df = pd.read_csv(url_white, sep=\";\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDtoFzrPmfEG"
      },
      "source": [
        "Let's do some brief exploratory data analysis to familiarize ourselves with the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9PrM3X3mfEG"
      },
      "outputs": [],
      "source": [
        "# DataFrame.head() let's us peek at the first few lines of our data table\n",
        "red_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMtKhxaemfEG"
      },
      "outputs": [],
      "source": [
        "# get more info about our DataFrame: data types and missing data!\n",
        "red_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwNqflb5mfEG"
      },
      "source": [
        "What does the full distribution of each feature look like?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HLrEMWImfEH"
      },
      "outputs": [],
      "source": [
        "sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(20,20)})\n",
        "\n",
        "# as histogram\n",
        "red_df.hist(bins=20,color='darkblue')\n",
        "\n",
        "# as boxplot\n",
        "#red_df.plot( kind = 'box', subplots = True, layout = (4,4), sharex = False, sharey = False, color='darkblue')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8MQZA3LmfEH"
      },
      "source": [
        "### What about these measurements, if anything, stands out?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pGmyouvmfEH"
      },
      "outputs": [],
      "source": [
        "# check how many high/low quality wines there actually are\n",
        "red_df['quality'].value_counts() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix66XIkfmfEH"
      },
      "source": [
        "# K-Nearest Neighbors (KNN) regression\n",
        "\n",
        "In practice we should think about our features more, but let's press forward and build a ML model to predict **quality**! \n",
        "\n",
        "Later, we'll step back and think about how we can improve the model, and in the process demonstrate common best practices in ML and show how they *actually* make a difference in model performance. \n",
        "\n",
        "These best practices apply to many other ML methods such as neural networks, but we'll use KNN to demonstrate.\n",
        "\n",
        "We've decided what our features and response are, let's create renamed data tables storing these variables to make life easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r6_IsYwmfEI"
      },
      "outputs": [],
      "source": [
        "response = red_df['quality']\n",
        "features = red_df.drop(['quality'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIdRXF_qmfEI"
      },
      "source": [
        "## Split data into training/testing subsets\n",
        "\n",
        "Let's use 70% of the data for training and 30% for testing, using `train_test_split` function (detailed description [here](https://cdn-coiao.nitrocdn.com/CYHudqJZsSxQpAPzLkHFOkuzFKDpEHGF/assets/static/optimized/rev-85bf93c/wp-content/uploads/2022/05/sklearn-train-test-split_syntax-explanation_v2.png)).\n",
        "\n",
        "In the context of KNN on these wine data: we'll use 70% of our data to make **quality** predictions about the other 30% that we held out. \n",
        "\n",
        "That is, for each sample in our test data, we'll find it's nearest neighbors in the training data and use these to make a prediction.\n",
        "\n",
        "We can then compare these predictions to the true values to assess performance!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JQ3kiEBmfEI"
      },
      "outputs": [],
      "source": [
        "# Here we will split our features (X) and response (Y) data into 2 categories each: _train and _test\n",
        "# We change names to X and Y as this is common in scikit learn tutorials and makes code shorter\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(features, response, test_size=0.30, random_state=11)\n",
        "print(\"X_train dimensions \", X_train.shape)\n",
        "print(\"X_test dimensions \", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_RUsC6gmfEI"
      },
      "source": [
        "Our data are ready for modeling.\n",
        "\n",
        "We'll first 'initialize' our KNN regression model by specifying some parameters. We have to make some choices:\n",
        "1. How many neighbors (*K*) should we use for regression?\n",
        "2. Of the *K* nearest neighbors, should ones further away be treated the same as those nearby?\n",
        "    - `uniform`: all neighbors equal\n",
        "    - `distance`: weight neighbors by inverse of distance from data point\n",
        "3. How do we measure distance between data points to calculate \"nearness\"? Let **d** be a vector of differences between a test sample and training sample, one element for each feature\n",
        "    - `l1`: L1 norm (Manhattan distance), $\\sum_{i=1}^{n} |d_i|$\n",
        "    - `l2`: L2 norm (Euclidean distance), $\\sqrt{\\sum_{i=1}^{n} d_{i}^2}$\n",
        "\n",
        "\n",
        "### How would you decide which options to use?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmCgCDs3mfEI"
      },
      "outputs": [],
      "source": [
        "# Initialize kNN; here you could e.g. use one of scikit learn's neural networks\n",
        "knn = KNeighborsRegressor(n_neighbors=3, weights=\"uniform\", metric='l2')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-4pZn5lmfEJ"
      },
      "source": [
        "Now we can give our model the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS_im--UmfEJ"
      },
      "outputs": [],
      "source": [
        "# Give our KNN model the training data for model fitting\n",
        "knn.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZcRoHdJmfEJ"
      },
      "source": [
        "Let's use this model to make predictions with our test data using the `.predict()` function and compare these with the true values. We will store all this information in another DataFrame and to analyze how we did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNEBFw-imfEJ"
      },
      "outputs": [],
      "source": [
        "# predict values from the test set, create a DataFrame \n",
        "predictions =  knn.predict(X_test)\n",
        "results = pd.DataFrame.from_dict({\"truth\" : Y_test, \"prediction\" : predictions})\n",
        "\n",
        "results.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9TqLFf9mfEJ"
      },
      "source": [
        "### Using this table, briefly examine how we did. \n",
        "\n",
        "Compute the mean absolute value of the differences between the truth and prediction columns. You can use the following functions:\n",
        "- abs()\n",
        "- np.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIS76TH6mfEK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jnhi7gnWmfEK"
      },
      "outputs": [],
      "source": [
        "# my detailed plot of regression performance\n",
        "\n",
        "# set some variables for creating a plot\n",
        "sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(10,5)})\n",
        "p = results.boxplot(column=\"prediction\", by=\"truth\", fontsize=15, \n",
        "                boxprops=dict(linestyle='-', linewidth=4),\n",
        "                medianprops=dict(linestyle='-', linewidth=4))\n",
        "p.set(xlabel=\"true value\", ylabel=\"predicted values\", title=\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtA5iho1mfEK"
      },
      "source": [
        "## Evalutate model\n",
        "\n",
        "Scikit-learn has a function to calculate an overall model [`score`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor.score:~:text=of%20the%20prediction.-,The%20coefficient%20of%20determination,score%20of%200.0.,-Parameters%3A) called the **coefficient of determination**:\n",
        "\n",
        "$\\huge R = 1-\\frac{\\sum_{i} (y_i - \\hat{y}_i)^2}{\\sum_{i} (y_i - \\bar{y}_i)^2}$\n",
        "\n",
        " \n",
        "The part on the right divides the sum of squared differences between our predictions ($\\hat{y}_i$) and true values ($y_i$) by the sum of squared differences between true values ($y_i$) and the overall mean ($\\bar{y}_i$). This is like comparing our 'fancy' predictions with the simplest predictor possible: assign every new sample the mean value. \n",
        "\n",
        "So, this score essentially compares by well we do relative to just using the mean. \n",
        "\n",
        "**1 is good, 0 is bad**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UWMuhvemfEK"
      },
      "outputs": [],
      "source": [
        "# score() returns the coefficient of determination of the prediction\n",
        "# note that our knn object has the training data already stored within it, so we only give it the test data here\n",
        "print(knn.score(X_test, Y_test))\n",
        "\n",
        "# or by hand, to show how it's calculated\n",
        "RSS = ((Y_test - knn.predict(X_test))** 2).sum() \n",
        "TSS = ((Y_test - Y_test.mean()) ** 2).sum()\n",
        "print(1-(RSS/TSS))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHs_wv5ZmfEK"
      },
      "source": [
        "At least it's not zero?\n",
        "\n",
        "This score was for a single parameterization of the KNN model. We can try other parameterizations as mentioned above:\n",
        "\n",
        "1. different *K* values (number of neighbors)\n",
        "2. different weight functions (`uniform` or `distance`)\n",
        "3. different distance metrics (`l1` or `l2`)\n",
        "\n",
        "Experiment with some other parameters using the cell below which re-trains and re-scores the model. Do other values make a difference? What's the highest value you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2XzDouSmfEL"
      },
      "outputs": [],
      "source": [
        "# Initialize, fit, predict\n",
        "knn = KNeighborsRegressor(n_neighbors=3, weights=\"uniform\", metric='l2')\n",
        "knn.fit(X_train, Y_train)\n",
        "print(knn.score(X_test, Y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SOy_K1kmfEL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmY8JPkDmfEL"
      },
      "source": [
        "We will tune our model by finding the combination of parameter values that gives us the highest score!\n",
        "\n",
        "To automate this model tuning, let's put this code for model fitting and scoring into a single function `get_scores` which we can give a list of *K* values, a weighting function, and a distance function. \n",
        "\n",
        "This function will then return a list of scores, one for each value of *K*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Uzw44EUmfEL"
      },
      "outputs": [],
      "source": [
        "def get_scores(x_train, \n",
        "                x_test, \n",
        "                y_train, \n",
        "                y_test, \n",
        "                k_values, \n",
        "                weight_function, \n",
        "                distance_metric):\n",
        "    \"\"\"\n",
        "    input:\n",
        "        k_values: a list of K nearest neighbors to use\n",
        "        w: the weight function to use\n",
        "        m: the distance metric to use\n",
        "    output:\n",
        "        a list of scores, one for each set of parameter values\n",
        "    \"\"\"\n",
        "    \n",
        "    test_scores = []    # a list that will eventually contain all the model scores\n",
        "    for k in k_values:\n",
        "        # initialize\n",
        "        knn = KNeighborsRegressor(n_neighbors=k, weights=weight_function, metric=distance_metric)\n",
        "        # train model\n",
        "        knn.fit(x_train, y_train)\n",
        "        # score/test model\n",
        "        test_scores.append(knn.score(x_test, y_test))\n",
        "    return test_scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uuC-GYcmfEL"
      },
      "source": [
        "Let's give `get_scores` a list of *K* values from 1 to 100. \n",
        "\n",
        "While we're at it, let's also compare the `uniform` and `distance` weighting functions.\n",
        "\n",
        "Thus, we will train and test 200 KNN models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5bWFAkRmfEL"
      },
      "outputs": [],
      "source": [
        "k_values = [i for i in range(1,101)]    # a list containing values 1 through 100\n",
        "\n",
        "scores_uniform = get_scores(X_train, X_test, Y_train, Y_test, k_values, weight_function = \"uniform\", distance_metric = \"l2\")\n",
        "scores_distance = get_scores(X_train, X_test, Y_train, Y_test, k_values, weight_function = \"distance\", distance_metric = \"l2\")\n",
        "\n",
        "# plot results\n",
        "sns.set(style='whitegrid',font_scale=1.3, rc={'figure.figsize':(5,5)})\n",
        "p = sns.lineplot(x=k_values, y=scores_uniform, color='darkblue')\n",
        "sns.lineplot(x=k_values, y=scores_distance, color='darkred')\n",
        "p.set(xlabel=\"k neighbors\", ylabel=\"score\", title=\"red:distance, blue:uniform\")\n",
        "\n",
        "print(\"Best uniform score: \", max(scores_uniform))\n",
        "print(\"Best distance score: \", max(scores_distance))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsQp09uqmfEL"
      },
      "source": [
        "Let's use \"distance\" for the **weights** parameter. \n",
        "\n",
        "What about for the **metric** parameter? Use code from the cell above, and instead of varying the weight function, vary the distance metric to test out the L1 norm `l1` and the L2 norm `l2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El-770YHmfEM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QrJx7gPmfEM"
      },
      "source": [
        "By constantly referring to test scores, we are using the data to make decisions about how to parameterize/configure our KNN model.\n",
        "\n",
        "---\n",
        "\n",
        "# Feature selection\n",
        "\n",
        "The success of KNN depends on the fact that samples that are nearby in feature space also have similar response values, such that we can use these KNN responses for accurate prediction. Stated another way, the values of the features are **correlated** with the reponse values. If this isn't the case, nearby neighbors might have wildly different reponse values, such that 'nearness' in feature space isn't informative for predicting reponse values, leading to inaccurate predictions!\n",
        "\n",
        "So far we've used all available features to make predictions, but some of these features may be useless.\n",
        "\n",
        "*At best*, including non-informative features makes our model more complex, less interpretable, more computationally costly to use and maintain (we will measure useless features for future data!). This is particularly problematic\n",
        "for neural networks, in which adding features can dramatically increase the number of parameters.\n",
        "\n",
        "*At worst*, non-informative features can ***reduce*** the model performance.\n",
        "\n",
        "Let's try to select only the more informative features by looking at the **Pearson correlation** between all of our variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT9IFNawmfEM"
      },
      "outputs": [],
      "source": [
        "sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(15,15)})\n",
        "# calculate pearson correlation between each pair of features\n",
        "pcorr = red_df.corr(method=\"pearson\")\n",
        "\n",
        "# matrix is symmetric, upper right half same as lower left half\n",
        "# not necessary, but for viz purposes let's mask the upper right half\n",
        "mask = np.zeros_like(pcorr)\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "# Generate a custom diverging colormap\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "sns.heatmap(pcorr, cmap=cmap, mask=mask, vmax=.3, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Skf5F0jmfEM"
      },
      "outputs": [],
      "source": [
        "# another way to qaulitatively inspect the relationships between our variables\n",
        "# warning: this function takes ~30 sec to run!\n",
        "#sns.pairplot(red_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTzUdRXomfEM"
      },
      "source": [
        "### How should we look at this plot? Which features should we select?\n",
        "While you ponder this, run the above cell that takes ~30 seconds to compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNTiaWJRmfEN"
      },
      "outputs": [],
      "source": [
        "# Select features that have at least a +/- 20% correlation with the response. \n",
        "# This is an unsophisticated way to conduct feature selection but will do for today!\n",
        "# Domain expertise of the problem/system is very useful at this stage.\n",
        "\n",
        "red_df_selection = red_df[['volatile acidity', 'citric acid', 'sulphates', 'alcohol', 'quality']]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUx-lmItmfEN"
      },
      "source": [
        "Let's repeat the same analyses above on this new DataFrame with selected features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WovGkCHwmfEN"
      },
      "outputs": [],
      "source": [
        "response = red_df_selection['quality']\n",
        "features = red_df_selection.drop(['quality'], axis=1)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(features, response, test_size=0.30, random_state=11)\n",
        "\n",
        "scores_l2 = get_scores(X_train, X_test, Y_train, Y_test, k_values, weight_function = \"distance\", distance_metric =\"l2\")\n",
        "scores_l1 = get_scores(X_train, X_test, Y_train, Y_test, k_values, weight_function = \"distance\", distance_metric =\"l1\")\n",
        "\n",
        "sns.set(style='whitegrid',font_scale=1.3, rc={'figure.figsize':(5,5)})\n",
        "p = sns.lineplot(x=k_values, y=scores_l2, color='darkblue')\n",
        "sns.lineplot(x=k_values, y=scores_l1, color='darkred')\n",
        "p.set(xlabel=\"k neighbors\", ylabel=\"score\", title=\"red:L1, blue:L2\")\n",
        "\n",
        "print(\"Best score: \", max(scores_l1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4-THhOUmfEN"
      },
      "source": [
        "Feature selection makes a difference in overall predictions! Also, it can potentially affect the performance of other parameters (e.g. weight function, distance metric), so feature selection should really be done early on in the ML modeling process, during the exploratory data analysis.\n",
        "\n",
        "# Feature Normalization\n",
        "\n",
        "If one feature has a range of values between [0-15] (e.g. alcohol content) and another has a range only between [0-0.5] (e.g. chlorides), then the range of distances between test and training samples will also be different for these two features, such that some features will contribute more or less to the regression/classification model.\n",
        "\n",
        "This essentially means we aren't using all the data available to us!\n",
        "\n",
        "We fix this by ***normalizing*** our features such that they are scaled similarly, with similar distributions and ranges.\n",
        "\n",
        "Let's first use the `DataFrame.describe()` function to get an idea of how the range of each selected feature varies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJEoFty0mfEN"
      },
      "outputs": [],
      "source": [
        "features.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4356URDOmfEN"
      },
      "source": [
        "\"Skewed\" distributions can have a similar impact on regression/classification, which we can inspect using `DataFrame.skew()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsaCzXRlmfEN"
      },
      "outputs": [],
      "source": [
        "features.skew()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwIxHdWOmfEO"
      },
      "source": [
        "To normalize, we'll use the standard Z score such that a feature's value will be transformed as $\\Large x' = \\frac{(x - \\mu)}{\\sigma}$.\n",
        "\n",
        "Another option is the min-max normalization $\\Large x' = \\frac{x - min(x)}{max(x) - min(x)}$\n",
        "\n",
        "### Which one do you think we should use?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEOChFUtmfEO"
      },
      "outputs": [],
      "source": [
        "# Initiate the scaler\n",
        "scaler = StandardScaler()\n",
        "# another option\n",
        "#scaler = MinMaxScaler()\n",
        "\n",
        "# Fit/transform to all numeric data\n",
        "features_transformed = scaler.fit_transform(features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gG9Wmk8mfEO"
      },
      "source": [
        "Let's confirm our features have actually been transformed as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Dr9EjPXmfEO"
      },
      "outputs": [],
      "source": [
        "sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(8,8)})\n",
        "pd.DataFrame(features_transformed).hist(bins=20,color='darkblue' )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsj0PxuomfEO"
      },
      "source": [
        "Let's rerun the regression with the transformed features to see if it increased scores. This is all the same code, except here we're using the `features_transformed` variable, not `features` as above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxxGx-D2mfEO"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(features_transformed, response, test_size=0.30, random_state=11)\n",
        "\n",
        "scores_l2 = get_scores(X_train, X_test, Y_train, Y_test, k_values, weight_function = \"distance\", distance_metric = \"l2\")\n",
        "scores_l1 = get_scores(X_train, X_test, Y_train, Y_test, k_values, weight_function = \"distance\", distance_metric = \"l1\")\n",
        "\n",
        "sns.set(style='whitegrid',font_scale=1.3, rc={'figure.figsize':(5,5)})\n",
        "p = sns.lineplot(x=k_values, y=scores_l2, color='darkblue')\n",
        "sns.lineplot(x=k_values, y=scores_l1, color='darkred')\n",
        "p.set(xlabel=\"k neighbors\", ylabel=\"score\", title=\"red:L1, blue:L2\")\n",
        "\n",
        "print(\"Best score: \", max(scores_l1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h1fSpGzmfEP"
      },
      "source": [
        "We get a score of ~0.5 with the L1 norm and using ~15 neighbors! A pretty nice increase from our initial score of ~0.17 above.\n",
        "\n",
        "Using code from above, we can again visualize the true and predicted values within the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGOypPaZmfEP"
      },
      "outputs": [],
      "source": [
        "knn.fit(X_train, Y_train)\n",
        "\n",
        "tmp = pd.DataFrame.from_dict({\"truth\" : Y_test, \"prediction\" : knn.predict(X_test)})\n",
        "sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(10,5)})\n",
        "p = tmp.boxplot(column=\"prediction\", by=\"truth\", fontsize=15, \n",
        "                boxprops=dict(linestyle='-', linewidth=4),\n",
        "                medianprops=dict(linestyle='-', linewidth=4))\n",
        "p.set(xlabel=\"true value\", ylabel=\"predicted values\", title=\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwSBILx3mfEP"
      },
      "source": [
        "Looks like we still systematically overestimate values of bad wines and underestimate values of good wines.\n",
        "\n",
        "----\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOUVzNLYmfEP"
      },
      "source": [
        "# K-Nearest Neighbors Classification\n",
        "Let's take our quality scores and convert them to labels to see if we can classify a wine as \"poor\", \"avertage\", or \"excellent\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mgzy7gAmfEP"
      },
      "outputs": [],
      "source": [
        "bins = [0, 4, 6, 10]\n",
        "labels = [\"poor\", \"average\", \"excellent\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRgg4C_zmfEP"
      },
      "source": [
        "We will use these numerical boundaries in `bins` to assign 1 of 3 labels to each wine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzquWdmAmfEP"
      },
      "outputs": [],
      "source": [
        "# create a new 'quality_label' column by binning 'quality' into three categories\n",
        "red_df_selection.loc[:,'quality_label'] = pd.cut(red_df_selection['quality'], bins=bins, labels=labels)\n",
        "\n",
        "# we no longer need the 'quality' column\n",
        "red_df_selection.drop('quality', axis =1, inplace = True)\n",
        "red_df_selection.head(n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAETTg1SmfEP"
      },
      "source": [
        "Do any of our features have information about our quality label? Type a feature into the `y=` argument of the `sns.boxplot` function. Options invlude those in the header we just printed above!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6D99Ru7mfEQ"
      },
      "outputs": [],
      "source": [
        "sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(5,5)})\n",
        "sns.boxplot(x='quality_label',y='', data=red_df_selection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbtm3GvjmfEQ"
      },
      "source": [
        "Here, I'm taking all the code from above and condensing it into one cell to see how classification performance changes with the number of K neighbors used.\n",
        "\n",
        "We will use scikit-learn's `KNeighborsClassifier` instead of `KNeighborsRegressor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iGawSIfmfEQ"
      },
      "outputs": [],
      "source": [
        "response = red_df_selection['quality_label']\n",
        "features = red_df_selection.drop(['quality_label'], axis=1)\n",
        "\n",
        "# feature normalization\n",
        "scaler = StandardScaler()\n",
        "features_transformed = scaler.fit_transform(features)\n",
        "\n",
        "# split data into training and testing sets\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(features_transformed, response, test_size=0.30, random_state=11)\n",
        "\n",
        "# create function to test the KNN classifier across a range of K values\n",
        "# note that this uses the 'accuracy_score' for classification, instead of 'score' as used above for regression\n",
        "# the 'accuracy_score' is simply the fraction of correct predictions\n",
        "def knn_classification_iterate(x_train, x_test, y_train, y_test, k_values, w, m):\n",
        "    test_scores = []\n",
        "    # For each k\n",
        "    for k in k_values:\n",
        "        # Initialize, fit, predict\n",
        "        knn = KNeighborsClassifier(n_neighbors=k, weights=w, metric=m)\n",
        "        knn.fit(x_train, y_train)\n",
        "        pred_knn = knn.predict(x_test)\n",
        "        accuracy_score(y_test, pred_knn)\n",
        "        test_scores.append(accuracy_score(Y_test, pred_knn))\n",
        "    return test_scores\n",
        "\n",
        "# Use the above function\n",
        "# note with KNN we may have ties, i.e. a test sample's 3 nearest neighbors are a poor, average, and excellent wine\n",
        "# 'breaking' these ties is a bit arbitrary, we will ignore this for now but in practice you should consider this!\n",
        "scores_l1 = knn_classification_iterate(X_train, X_test, Y_train, Y_test, k_values, w = \"distance\", m=\"l1\")\n",
        "\n",
        "# plot accuracy score results\n",
        "sns.set(style='whitegrid',font_scale=1.3, rc={'figure.figsize':(5,5)})\n",
        "p = sns.lineplot(x=k_values, y=scores_l1, color='darkblue')\n",
        "p.set(xlabel=\"k neighbors\", ylabel=\"accuracy\", title=\"\")\n",
        "\n",
        "print(\"Best score: \", max(scores_l1))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URx7d0czmfEQ"
      },
      "source": [
        "Greater than 88% accuracy looks nice!\n",
        "\n",
        "However, this is a little misleading. For instance, let's revisit the distribution of quality scores in our data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GG7aZyISmfEQ"
      },
      "outputs": [],
      "source": [
        "red_df_selection['quality_label'].value_counts(normalize=True)\n",
        "Y_test.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khWusOWqmfEQ"
      },
      "source": [
        "**Most** of our wines are 'average', such we could build a model without any features, labelling every single new sample as 'average', and our accuracy would be $\\frac{395}{395+65+20} = 0.823$\n",
        "\n",
        "Our accuracy scores of ~0.88 are certainly better, but this extremely simple approach already get us most of the way there...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHa_91M3mfEQ"
      },
      "source": [
        "Alternatively, say we are very interested in predicting 'excellent' wines and we are concerned about our 'excellent' predictions being accurate. The simple approach would be label each new wine as excellent, which would create a bad model with $\\frac{65}{395+65+20} = 0.135$ accuracy. How does the accuracy our our 'excellent' predictions compare to this?\n",
        "\n",
        "Let's create a [**confusion matrix**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html), where rows are the true labels and columns are the predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai7myh3zmfER"
      },
      "outputs": [],
      "source": [
        "# make a classifier that's optimized according to our results above, with k=18 neighbors\n",
        "knn = KNeighborsClassifier(n_neighbors=18, weights='distance', metric='l1')\n",
        "knn.fit(X_train, Y_train)\n",
        "pred_knn_for_best_k = knn.predict(X_test)\n",
        "\n",
        "# give confusion_matrix true values and predicted values to make a table\n",
        "cm = confusion_matrix(Y_test, pred_knn_for_best_k)\n",
        "\n",
        "# print the matrix\n",
        "names = [\"average\", \"excellent\", \"poor\"]\n",
        "print(pd.DataFrame(cm, index=names, columns=names))\n",
        "# rows are true labels, columns are predicted labels?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoKUG3lEmfER"
      },
      "source": [
        "The accuracy we calculated above divides the sum of all the entries in the diagonal of this matrix (true positives) by all entries in the matrix (true positives and false positives)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riW6UEfymfER"
      },
      "outputs": [],
      "source": [
        "diagonal = (383+39+1)\n",
        "all = (383+39+1+12+26+19)\n",
        "\n",
        "diagonal/all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qF-tFDFlmfER"
      },
      "source": [
        "Of our $39+12=51$ excellent predictions (sum of entries in the 'excellent' column), $\\frac{39}{51} = 0.76$ are correct, which is much better than ~$13$% from randomly guessing based on the proportion of excellent wines in the training data. Note that we missed 26 'excellent' wines that our classifier throught were 'average'.\n",
        "\n",
        "These concepts are known as **specificity** and **recall**, and will be covered in more detail later in the workshop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuNZhHAlmfER"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.5 ('datascience')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "74a539bb1a30b12322b0673657a3362ba97f8c77e4e9b57fd18ae9f35996046a"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/PrincetonUniversity/intro_machine_learning/blob/main/day1/ML_overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m93quVWjYEC3"
   },
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import rc\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Configure plotting\n",
    "rc('animation', html='jshtml')\n",
    "qualitative_colors = ['#1b9e77','#d95f02','#7570b3','#e7298a'] # four color-blind friendly qualitative colors, and black\n",
    "\n",
    "\n",
    "# Set the random seed\n",
    "rng = np.random.RandomState(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTHyH5eFeJAS"
   },
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlsfScOT4_4t"
   },
   "source": [
    "## Load Some Example Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l08xjoTfYViZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRPwZfz8YXTT",
    "outputId": "a694816e-940a-47ab-8b6f-1db457e3079b"
   },
   "outputs": [],
   "source": [
    "# Make the data set.\n",
    "diabetes_bunch = load_diabetes()\n",
    "\n",
    "print(diabetes_bunch.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sh32Dl4_n8K",
    "outputId": "453ac557-addb-4e44-c7b5-a93e9ab8bab2"
   },
   "outputs": [],
   "source": [
    "diabetes_X = diabetes_bunch.data\n",
    "diabetes_y = diabetes_bunch.target\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]\n",
    "\n",
    "print(diabetes_X_train.shape, diabetes_X_test.shape)\n",
    "print(diabetes_y_train.shape, diabetes_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 776
    },
    "id": "c34XfB1dYeOQ",
    "outputId": "673567ff-d052-402f-c44e-35ed4ae82c18"
   },
   "outputs": [],
   "source": [
    "# Plot the data set.\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200)\n",
    "axs.scatter(diabetes_X_train, diabetes_y_train, color=qualitative_colors[0], s=10)\n",
    "axs.set_xlabel('BMI (scaled)')\n",
    "axs.set_ylabel('quantitative measure of diabetes progression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGKO7-SeYogp"
   },
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0e9vwetcSSc"
   },
   "source": [
    "\n",
    "#### A straight line with input $x$ and output $y$ has the form $y = w_1 x + w_0$, where $w_0$ and $w_1$ are coefficients we aim to learn.\n",
    "\n",
    "#### We use the letter $w$ because we think of the coefficients as **weights**; the value of $y$ is changed by changing the relative weight of one term or another.\n",
    "\n",
    "#### The task of finding the function that best fits the training set of $n$ points in the $x$, $y$ plane is called **linear regression**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwO51_HPZ9wg"
   },
   "source": [
    "#### To fit the line to the data, all we have to do is find the values of the weights ($w_0 , w_1$) that minimize the *loss*.\n",
    "\n",
    "#### One way is to use the squared-error loss function:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(w_0, w_1)\n",
    "  &= \\sum_{i=1}^N (y_i - \\hat{y}_i)^2 \\\\\n",
    "  &= \\sum_{i=1}^{N} (y_i - (w_1 x_i + w_0))^2\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvvTnsYVZ_Ql"
   },
   "outputs": [],
   "source": [
    "def squarederror_loss(xs, ys, w0, w1):\n",
    "  \"\"\"Caculate the squared-error loss\n",
    "    Parameters\n",
    "    ----------\n",
    "    xs : array_like\n",
    "        x-axis values of data points, shape (number of data points)\n",
    "    ys : array_like\n",
    "        y-axis values of data points, shape (number of data points)\n",
    "    w0 : array_like\n",
    "        weight for intercept, shape (number of weights)\n",
    "    w1 : array_like\n",
    "        weight for slope, shape (number of weights)\n",
    "    Returns\n",
    "    -----------\n",
    "    loss : array_like\n",
    "        squared-error loss, shape (number of weights)\n",
    "    \"\"\"\n",
    "  xs = np.asarray(xs).flatten()\n",
    "  ys = np.asarray(ys).flatten()\n",
    "  loss = np.sum((ys[:,np.newaxis] - (w1[np.newaxis,:]*xs[:,np.newaxis] + w0[np.newaxis,:]))**2.,axis=0)\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6RrHTrKZ_3G"
   },
   "source": [
    "#### The squared-error loss function is minimized when the partial derivatives with respect to $ w_0 $ and $w_1 $ are zero:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial w_0} \\sum_{i=1}^{N} (y_i - (w_1 x_i + w_0))^2 = 0 $$\n",
    "\n",
    "$$ \\frac{\\partial}{\\partial w_1} \\sum_{i=1}^{N} (y_i - (w_1 x_i + w_0))^2 = 0$$\n",
    "\n",
    "#### This has unique solutions:\n",
    "\n",
    "\\begin{align}\n",
    "w_0 &= \\frac{1}{N} \\sum_{i=1}^N y_i - \\frac{w_1}{N} \\sum_{i=1}^N x_i \\\\\n",
    "&= \\bar{y} - w_1 \\bar{x}\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "w_1 &= \\frac{\\frac{1}{N} (\\sum_{i=1}^N x_i y_i) - \\bar{x} \\bar{y}}{\\frac{1}{N}(\\sum_{i=1}^N x^2_i) - \\bar{x}^2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tv4OHTXcbcB8"
   },
   "outputs": [],
   "source": [
    "def univariate_linear_regression(xs, ys):\n",
    "  \"\"\"Calculate optimal weights in the univariate linear regression case.\n",
    "    Parameters\n",
    "    ----------\n",
    "    xs : array_like\n",
    "        x-axis values of data points, shape (number of data points)\n",
    "    ys : array_like\n",
    "        y-axis values of data points, shape (number of data points)\n",
    "    Returns\n",
    "    -----------\n",
    "    w0 : float\n",
    "        weight for intercept\n",
    "    w1 : float\n",
    "        weight for slope\n",
    "    \"\"\"\n",
    "  xs = np.asarray(xs).flatten()\n",
    "  ys = np.asarray(ys).flatten()\n",
    "  N = float(xs.shape[0])\n",
    "  w1 = (np.sum(xs*ys)/N - np.mean(xs)*np.mean(ys) ) / ( np.sum(xs**2.)/N - np.mean(xs)**2.)\n",
    "  w0 = (np.mean(ys) - w1*np.mean(xs))\n",
    "  return w0, w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfgPXggTecbg",
    "outputId": "246c7c2c-179d-4d78-b6c9-0e2f0de60db6"
   },
   "outputs": [],
   "source": [
    "w0, w1 = univariate_linear_regression(diabetes_X_train, diabetes_y_train)\n",
    "print(w0, w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 776
    },
    "id": "xeEzVs7AelWv",
    "outputId": "07f799c8-837c-4302-ffb8-ac0ef0b986e2"
   },
   "outputs": [],
   "source": [
    "# Plot the data set with the linear regression fit.\n",
    "\n",
    "plot_xs = np.linspace(diabetes_X_train.min(), diabetes_X_train.max(), 101)\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200)\n",
    "axs.scatter(diabetes_X_train, diabetes_y_train, s=1, color=qualitative_colors[0])\n",
    "axs.plot(plot_xs, w1*plot_xs + w0, color=qualitative_colors[1], linewidth=2)\n",
    "axs.set_xlabel('BMI (scaled)')\n",
    "axs.set_ylabel('quantitative measure of diabetes progression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZRNB12c7zlU"
   },
   "source": [
    "#### We don't have to just use this optimal weight value, we can explore the weight space.\n",
    "\n",
    "#### We'll use the *squarederror_loss* function we defined earlier to calcule the loss over a wide range of $w_0$ and $w1$ values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGvaHZIzfJJl"
   },
   "outputs": [],
   "source": [
    "# Define a range of w0 and w1 values to calculate.\n",
    "grid_dim = 200\n",
    "w0_range = np.linspace(w0 - grid_dim, w0 + grid_dim, 200)\n",
    "w1_range = np.linspace(w1 - grid_dim, w1 + grid_dim, 200)\n",
    "\n",
    "# Make a 200 x 200 grids of w0 and w1 values over that range.\n",
    "XX, YY = np.meshgrid(w0_range, w1_range)\n",
    "\n",
    "# Flatten the grids to lists with 40000 values.\n",
    "XY = np.c_[XX.ravel(), YY.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3iUDRt0Ghjej"
   },
   "outputs": [],
   "source": [
    "# Calculate the squared-error loss for each pair of w0, w1 values.\n",
    "Z = squarederror_loss(diabetes_X_train, diabetes_y_train, XY[:,0], XY[:,1])\n",
    "\n",
    "# Turn the list of loss values into a 200 x 200 grid.\n",
    "Z = Z.reshape(XX.shape)\n",
    "\n",
    "# Also calculate the loss for the optimal w0, w1 values we found earlier.\n",
    "best_fit_loss = squarederror_loss(diabetes_X_train, diabetes_y_train, np.array([w0]), np.array([w1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 776
    },
    "id": "nDbg5iU1i2AK",
    "outputId": "93f2392d-f966-4e36-b618-66375ac2ac22"
   },
   "outputs": [],
   "source": [
    "# Plot the squared-error loss values.\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200, sharex=True)\n",
    "CS = axs.contour(XX, YY, Z)\n",
    "axs.clabel(CS, inline=True, fontsize=10, fmt='%d')\n",
    "axs.scatter(w0, w1, s=10, color='k')\n",
    "axs.set_ylabel(r'$w_1$')\n",
    "axs.set_xlabel(r'$w_0$')\n",
    "\n",
    "print(best_fit_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWYEQpbTlsz6"
   },
   "source": [
    "#### This is an ideal case, where it is easy to find an optimal solution where the partial derivatives are zero.\n",
    "\n",
    "#### What are other methods for minimizing loss that does not depend  on solving partial derivatives and can be applied to any loss function?\n",
    "\n",
    "#### We can search through a continuous weight space looking for the minimum, using a technique called *gradient descent*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtdVf8q59bvq"
   },
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lIzpihvv9z-I"
   },
   "source": [
    "### Step 0\n",
    "\n",
    "#### Choose a starting point in the weight space - an initial value of $w_0$ and $w_1$.\n",
    "\n",
    "#### A simple method to do this is to randomly choose a point, but this could be very far from the optimal position.\n",
    "\n",
    "#### Usually you'll have some intuition about your data and will choose several values close to what you expect and rerun the procedure to see if you get the same answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iqjeu9wK-Odi"
   },
   "outputs": [],
   "source": [
    "w0_init, w1_init = w0-10., w1-10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqVG_K4V-R80"
   },
   "source": [
    "### Step 1\n",
    "\n",
    "#### Compute an estimate of the gradient at this point.\n",
    "\n",
    "#### For the univariate case:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial w_0} \\sum_{i=1}^{n} (y_i - (w_1 x_i + w_0))^2 &= -2 \\sum_{i=1}^{n}(y_i - (w_1 x_i + w_0)) \\\\\n",
    "\\frac{\\partial}{\\partial w_1} \\sum_{i=1}^{n} (y_j - (w_1 x_i + w_0))^2 &= -2 \\sum_{i=1}^{n}(y_j - (w_1 x_i + w_0)) x_i\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iE5HndSl_Ofl"
   },
   "outputs": [],
   "source": [
    "def gradient_of_weights(xs, ys, w0, w1):\n",
    "  \"\"\"Calculate the partial derivative of the loss function with respect to the weights.\n",
    "  Parameters\n",
    "  ----------\n",
    "  xs : array_like\n",
    "      x-axis values of data points, shape (number of data points)\n",
    "  ys : array_like\n",
    "      y-axis values of data points, shape (number of data points)\n",
    "  w0 : float\n",
    "    weight for intercept\n",
    "  w1 : float\n",
    "      weight for slope\n",
    "  Returns\n",
    "  -----------\n",
    "  derivative_w0 : float\n",
    "      partial derivative of the loss function with respect to theweight for slope\n",
    "  derivative_w1 : float\n",
    "      partial derivative of the loss function with respect to the weight for slope\n",
    "  \"\"\"\n",
    "\n",
    "  derivative_w0 = np.sum((ys - (w1*xs + w0)) )\n",
    "  derivative_w1 = np.sum((ys - (w1_init*xs + w0_init))*xs)\n",
    "\n",
    "  return derivative_w0, derivative_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WmB_KN5AlaI"
   },
   "source": [
    "### Step 2\n",
    "\n",
    "#### Move a small amount from the initial value in the steepest downhill direction.\n",
    "\n",
    "#### We specfic the *small amount* to move as $\\alpha$. This is often called the *learning rate* or *step size*.\n",
    "\n",
    "#### The initial $w_0$ and $w_1$ are updated in the following way:\n",
    "\n",
    "\\begin{align}\n",
    "w_0 &← w_0 + \\alpha \\sum_{i=1}^{n}(y_j - (w_1 x_i + w_0)) \\\\\n",
    "w_1 &← w_1 + \\alpha \\sum_{i=1}^{n}(y_j - (w_1 x_i + w_0)) x_i\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ABHuD3blBlwp"
   },
   "outputs": [],
   "source": [
    "def update_weights(xs, ys, w0_old, w1_old, step_size):\n",
    "  \"\"\" Update the weights using the partial derivate and step size.\n",
    "  Parameters\n",
    "  ----------\n",
    "  xs : array_like\n",
    "      x-axis values of data points, shape (number of data points)\n",
    "  ys : array_like\n",
    "      y-axis values of data points, shape (number of data points)\n",
    "  w0_old : float\n",
    "    weight for intercept\n",
    "  w1_old : float\n",
    "      weight for slope\n",
    "  Returns\n",
    "  -----------\n",
    "  w0_new: float\n",
    "    weight for intercept\n",
    "  w1_new : float\n",
    "    weight for slope\n",
    "  \"\"\"\n",
    "  xs = np.asarray(xs).flatten()\n",
    "  ys = np.asarray(ys).flatten()\n",
    "  w0_new = w0_old + step_size*np.sum((ys - (w1_old*xs + w0_old)) )\n",
    "  w1_new = w1_old + step_size*np.sum((ys - (w1_old*xs + w0_old))*xs)\n",
    "  return w0_new, w1_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvorriZFCVMh"
   },
   "source": [
    "### Repeat Steps 1 - 2\n",
    "\n",
    "### until difference between the old and the new weights is sufficiently small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-NqtcDFkZ5p"
   },
   "outputs": [],
   "source": [
    "def univariate_gradient_decent(xs, ys, w0_init, w1_init, step_size = 0.001, sufficiently_small = 0.00001):\n",
    "  \"\"\" Update the weights using the partial derivate and step size.\n",
    "  Parameters\n",
    "  ----------\n",
    "  xs : array_like\n",
    "      x-axis values of data points, shape (number of data points)\n",
    "  ys : array_like\n",
    "      y-axis values of data points, shape (number of data points)\n",
    "  w0_init : float\n",
    "    initial guess of weight for intercept\n",
    "  w1_init : float\n",
    "    initial guess of weight for slope\n",
    "  Returns\n",
    "  -----------\n",
    "  w0s: array_like\n",
    "    list of weights for intercept\n",
    "  w1s : array_like\n",
    "    list of weights for slope\n",
    "  \"\"\"\n",
    "  w0_firststep, w1_firststep = update_weights(xs, ys, w0_init, w1_init, step_size)\n",
    "\n",
    "  w0s = np.array( [w0_init, w0_firststep] )\n",
    "  w1s = np.array( [w1_init, w1_firststep] )\n",
    "\n",
    "  while np.any([np.abs(w0s[-2]-w0s[-1]) > sufficiently_small, np.abs(w1s[-2]-w1s[-1]) > sufficiently_small]):\n",
    "    w0_nextstep, w1_nextstep = update_weights(xs, ys, w0s[-1], w1s[-1], step_size)\n",
    "    w0s = np.append(w0s, w0_nextstep)\n",
    "    w1s = np.append(w1s, w1_nextstep)\n",
    "  return w0s, w1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g-jrb-dgJWji",
    "outputId": "42f0e451-9598-4467-ad67-2b60e215c964"
   },
   "outputs": [],
   "source": [
    "print(w0, w1)\n",
    "print(w0_init, w1_init)\n",
    "print(update_weights(diabetes_X_train, diabetes_y_train, w0_init, w1_init, step_size = 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QEvHSjHRrTnv"
   },
   "outputs": [],
   "source": [
    "weights_to_plot0, weights_to_plot1 = univariate_gradient_decent(diabetes_X_train, diabetes_y_train, w0_init, w1_init, sufficiently_small=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 776
    },
    "id": "nUYTyh2arWn7",
    "outputId": "73b18031-9412-45d1-b141-b9750afdd996"
   },
   "outputs": [],
   "source": [
    "# Plot the squared-error loss values.\n",
    "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200, sharex=True)\n",
    "CS = axs.contour(XX, YY, Z)\n",
    "axs.clabel(CS, inline=True, fontsize=10, fmt='%d')\n",
    "axs.plot(weights_to_plot0, weights_to_plot1, '-', color='k')\n",
    "axs.scatter(w0, w1, s=10, color='k')\n",
    "axs.set_ylabel(r'$w_1$')\n",
    "axs.set_xlabel(r'$w_0$')\n",
    "axs.set_xlim(w0-20., w0+20.)\n",
    "axs.set_ylim(w1-20., w1+20.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKJbfPQvb4fc"
   },
   "source": [
    "#### This is a very simple example, in just two dimensions, but it can extend to many dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_14ttgycQWv"
   },
   "outputs": [],
   "source": [
    "# Load the full diabetes dataset\n",
    "diabetes_dataset = load_diabetes(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eBw9q1bpeYob",
    "outputId": "f7dd6a28-a4d2-4d93-9148-2ddd74f7114a"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(diabetes_dataset.frame, corner=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HIjhLfw4G6Gm",
    "outputId": "b11c41c0-53a5-422c-9cf9-529a1c89aa47"
   },
   "outputs": [],
   "source": [
    "features = diabetes_dataset.data\n",
    "print(features.columns)\n",
    "features = features.drop(columns=['sex'])\n",
    "print(features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isMTsIkPb9uW"
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3-mKJgDVcAdY",
    "outputId": "92cc108f-fd6b-46d2-cd48-1b0afb6f907b"
   },
   "outputs": [],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(features, diabetes_dataset.target)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mtWGy9MqEquf",
    "outputId": "4a3bf19b-314b-4d2a-ba4a-9a4b1ed5ab38"
   },
   "outputs": [],
   "source": [
    "reg = linear_model.SGDRegressor(loss='squared_error', max_iter=10000)\n",
    "reg.fit(features, diabetes_dataset.target)\n",
    "print(reg.coef_)\n",
    "print(reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gn4WD4IJeCac"
   },
   "source": [
    "# Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuGcyzsxHy64"
   },
   "source": [
    "## Load Some Example Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKVh1m2SvWgM"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFQjaUlozXxM"
   },
   "outputs": [],
   "source": [
    "# Make the data set.\n",
    "X, labels = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=0, n_clusters_per_class=1, class_sep=3.)\n",
    "X += 2 * rng.uniform(size=X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 776
    },
    "id": "zuSzGYAgzebb",
    "outputId": "c6fe2e4b-c7f5-46bc-9270-5faa5389647d"
   },
   "outputs": [],
   "source": [
    "# Plot the data set.\n",
    "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200)\n",
    "axs.scatter(X[:, 0], X[:, 1], marker=\"o\", c=labels, s=25, edgecolor=\"k\")\n",
    "axs.set_xlabel('$x_1$')\n",
    "axs.set_ylabel('$x_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNmZUDPcn28L"
   },
   "source": [
    "## Decision Boundary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jt7g8F9GcXJD"
   },
   "source": [
    "#### A *decision boundary* is a line (or a hyperplane in more than two dimensions) that separates two classes.\n",
    "#### Linearly separable data can be divided using a linear decision boundary:\n",
    "\n",
    "## $ x_2 - (w_1 x_1 + w_0) = 0 $\n",
    "\n",
    "#### The yellow points, which we want to classify with value 1, are above this line; they are points for which:\n",
    "## $ x_2 - (w_1 x_1 + w_0) > 0 $,\n",
    "#### while the purple points, which we want to classify with value 0, are\n",
    "## $ x_2 - (w_1 x_1 + w_0) < 0 $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 781
    },
    "id": "KZbPdNeIJ9a5",
    "outputId": "9d15eeae-3f23-4133-9cd8-6c45f50df5fd"
   },
   "outputs": [],
   "source": [
    "# Plot the data set and inital decision boundary guess.\n",
    "\n",
    "plot_xs = np.linspace(X[:, 0].min(), X[:, 0].max(), 101)\n",
    "\n",
    "w0_init, w1_init = -2., 2.\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200)\n",
    "axs.scatter(X[:, 0], X[:, 1], marker=\"o\", c=labels, s=25, edgecolor=\"k\")\n",
    "axs.plot(plot_xs, w1_init*plot_xs + w0_init, color='k', linewidth=2)\n",
    "axs.set_xlabel('$x_1$')\n",
    "axs.set_ylabel('$x_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUWEyDBpMAZr"
   },
   "source": [
    "#### Define a function that predicts the class based on the point's position relative to the decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8ktUyEwLCL5"
   },
   "outputs": [],
   "source": [
    "def predict_class(x1s, x2s, w0, w1):\n",
    "  \"\"\" Predict the class of each point.\n",
    "  Parameters\n",
    "  ----------\n",
    "  x1s : array_like\n",
    "      x-axis values of data points, shape (number of data points)\n",
    "  x2s : array_like\n",
    "      y-axis values of data points, shape (number of data points)\n",
    "  w0 : float\n",
    "    weight for intercept\n",
    "  w1 : float\n",
    "    weight for slope\n",
    "  Returns\n",
    "  -----------\n",
    "  prediction: array_like\n",
    "    list of predicted classes\n",
    "  \"\"\"\n",
    "  prediction = np.asarray((x2s - (w1*x1s + w0)) > 0, dtype=int)\n",
    "  return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-W4sU7HALn9b",
    "outputId": "753fabb8-f3d8-47a9-fc52-bcadaf89e0f8"
   },
   "outputs": [],
   "source": [
    "predictions = predict_class(X[:, 0], X[:, 1], w0_init, w1_init)\n",
    "print(predictions)\n",
    "print(predictions == labels)\n",
    "print(f\"{np.sum(predictions == labels)} correct out of {labels.shape[0]} total\")\n",
    "print(f\"{np.sum(predictions == labels) / labels.shape[0] * 100 :0.1f}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FqyowdxHq79m"
   },
   "source": [
    "## Classification with Sci-Kit Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9NP1D4orGz2"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVV1WiI_tDb2",
    "outputId": "067acd28-dd5e-47ae-a5f3-24428114968d"
   },
   "outputs": [],
   "source": [
    "clf = SGDClassifier(loss='log_loss', max_iter=100)\n",
    "clf.fit(X, labels)\n",
    "print(clf.coef_[0])\n",
    "print(clf.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 776
    },
    "id": "uEw_Z0z0rK6R",
    "outputId": "e1760721-ccd9-4743-d48e-6a200cfd7d7d"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(figsize=(4.,4.), nrows=1, ncols=1, facecolor='white', dpi=200)\n",
    "axs.scatter(X[:, 0], X[:, 1], marker=\"o\", c=labels, s=25, edgecolor=\"k\")\n",
    "axs.plot(plot_xs, (-(plot_xs * clf.coef_[0, 0]) - clf.intercept_[0]) / clf.coef_[0, 1], color='k', linewidth=2)\n",
    "axs.set_xlabel('$x_1$')\n",
    "axs.set_ylabel('$x_2$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UugYZ-uNHv7L"
   },
   "source": [
    "# Data Collected from Red and White Wines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_LQ4w2gmfEB"
   },
   "outputs": [],
   "source": [
    "# import some more packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "66qiL7xKmfED"
   },
   "source": [
    "Our wine data has the following variables:\n",
    "\n",
    "----\n",
    "\n",
    "1. **fixed acidity**: Most acids in wine are \"fixed\" or nonvolatile (do not evaporate readily).\n",
    "2. **volatile acidity**: The amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste.\n",
    "3. **citric acid**: Found in small quantities, citric acid can add 'freshness' and flavor to wines.\n",
    "4. **residual sugar**: The amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter.\n",
    "5. **chlorides**: The amount of salt in the wine.\n",
    "6. **free sulfur dioxide**: The free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents.\n",
    "7. **quality**: based on sensory data, score between 0 and 10.\n",
    "8. **density**: The density of water is close to that of water depending on the percent alcohol and sugar content.\n",
    "9. **pH**: Describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4.\n",
    "10. **sulphates**: A wine preservative which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial.\n",
    "11. **alcohol**: The percent alcohol content of the wine.\n",
    "12. **total sulfur dioxide**: Amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2.\n",
    "\n",
    "----\n",
    "\n",
    "### Which variable should be our ***response***, with the rest being ***features***? Sometimes it's not obvious and depends on your interests!\n",
    "\n",
    "Load data into a Pandas [DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) using the `read_csv()` function, which can read a table on your computer are directly from a URL!."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hs4Yp0k5mfEF"
   },
   "outputs": [],
   "source": [
    "url_red = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "url_white = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
    "\n",
    "# these data are in text tables in which the separation of columns is indicated with a semicolon \";\"\n",
    "red_df = pd.read_csv(url_red, sep=\";\")\n",
    "white_df = pd.read_csv(url_white, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDtoFzrPmfEG"
   },
   "source": [
    "Let's do some brief exploratory data analysis to familiarize ourselves with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "w9PrM3X3mfEG",
    "outputId": "609578b4-4e1c-49a3-e2f7-c2efccb4e9ec"
   },
   "outputs": [],
   "source": [
    "# DataFrame.head() let's us peek at the first few lines of our data table\n",
    "red_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMtKhxaemfEG",
    "outputId": "13345105-096c-48ba-8e50-5a63c2523db7"
   },
   "outputs": [],
   "source": [
    "# get more info about our DataFrame: data types and missing data!\n",
    "red_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BwNqflb5mfEG"
   },
   "source": [
    "What does the full distribution of each feature look like?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "-HLrEMWImfEH",
    "outputId": "9949406f-0e6b-41f1-c9f7-caf6dff1b18d"
   },
   "outputs": [],
   "source": [
    "sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(20,20)})\n",
    "\n",
    "# as histogram\n",
    "red_df.hist(bins=20,color='darkblue')\n",
    "\n",
    "# as boxplot\n",
    "# red_df.plot( kind = 'box', subplots = True, layout = (4,4), sharex = False, sharey = False, color='darkblue')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8MQZA3LmfEH"
   },
   "source": [
    "### What about these measurements, if anything, stands out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3pGmyouvmfEH",
    "outputId": "941277cc-ece3-4aa4-886b-352f3dd164d3"
   },
   "outputs": [],
   "source": [
    "# check how many high/low quality wines there actually are\n",
    "red_df['quality'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ix66XIkfmfEH"
   },
   "source": [
    "# K-Nearest Neighbors (KNN) regression\n",
    "\n",
    "In practice we should think about our features more, but let's press forward and build a ML model to predict **quality**!\n",
    "\n",
    "Later, we'll step back and think about how we can improve the model, and in the process demonstrate common best practices in ML and show how they *actually* make a difference in model performance.\n",
    "\n",
    "These best practices apply to many other ML methods such as neural networks, but we'll use KNN to demonstrate.\n",
    "\n",
    "We've decided what our features and response are, let's create renamed data tables storing these variables to make life easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-r6_IsYwmfEI"
   },
   "outputs": [],
   "source": [
    "response = red_df['quality']\n",
    "features = red_df.drop(['quality'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sIdRXF_qmfEI"
   },
   "source": [
    "## Split data into training/testing subsets\n",
    "\n",
    "Let's use 70% of the data for training and 30% for testing, using `train_test_split` function (detailed description [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)).\n",
    "\n",
    "In the context of KNN on these wine data: we'll use 70% of our data to make **quality** predictions about the other 30% that we held out.\n",
    "\n",
    "That is, for each sample in our test data, we'll find it's nearest neighbors in the training data and use these to make a prediction.\n",
    "\n",
    "We can then compare these predictions to the true values to assess performance!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5JQ3kiEBmfEI",
    "outputId": "488d259a-5234-4006-9b46-6140ea4a2f5f"
   },
   "outputs": [],
   "source": [
    "# Here we will split our features (X) and response (Y) data into 2 categories each: _train and _test\n",
    "# We change names to X and Y as this is common in scikit learn tutorials and makes code shorter\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, response, test_size=0.30, random_state=11)\n",
    "print(\"X_train dimensions \", X_train.shape)\n",
    "print(\"X_test dimensions \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_RUsC6gmfEI"
   },
   "source": [
    "Our data are ready for modeling.\n",
    "\n",
    "We'll first 'initialize' our KNN regression model by specifying some parameters. We have to make some choices:\n",
    "1. How many neighbors (*K*) should we use for regression?\n",
    "2. Of the *K* nearest neighbors, should ones further away be treated the same as those nearby?\n",
    "  - `uniform`: all neighbors equal\n",
    "  - `distance`: weight neighbors by inverse of distance from data point\n",
    "3. How do we measure distance between data points to calculate \"nearness\"? Let **d** be a vector of differences between a test sample and training sample, one element for each feature\n",
    "  - `l1`: L1 norm (Manhattan distance), $\\sum_{i=1}^{n} |d_i|$\n",
    "  - `l2`: L2 norm (Euclidean distance), $\\sqrt{\\sum_{i=1}^{n} d_{i}^2}$\n",
    "\n",
    "\n",
    "### How would you decide which options to use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmCgCDs3mfEI"
   },
   "outputs": [],
   "source": [
    "# Initialize kNN; here you could e.g. use one of scikit learn's neural networks\n",
    "knn = KNeighborsRegressor(n_neighbors=3, weights=\"uniform\", metric='l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-4pZn5lmfEJ"
   },
   "source": [
    "Now we can give our model the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "jS_im--UmfEJ",
    "outputId": "1ba5ea23-7a60-4971-b14a-19ccc5f69571"
   },
   "outputs": [],
   "source": [
    "# Give our KNN model the training data for model fitting\n",
    "knn.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZcRoHdJmfEJ"
   },
   "source": [
    "Let's use this model to make predictions with our test data using the `.predict()` function and compare these with the true values. We will store all this information in another DataFrame and to analyze how we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "jNEBFw-imfEJ",
    "outputId": "dc53c05d-4434-4d4b-b902-74ea0f0bbcae"
   },
   "outputs": [],
   "source": [
    "# predict values from the test set, create a DataFrame\n",
    "predictions =  knn.predict(X_test)\n",
    "results = pd.DataFrame.from_dict({\"truth\" : Y_test, \"prediction\" : predictions})\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9TqLFf9mfEJ"
   },
   "source": [
    "### Using this table, briefly examine how we did.\n",
    "\n",
    "Compute the mean absolute value of the differences between the truth and prediction columns. You can use the following functions:\n",
    "- abs()\n",
    "- np.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LIS76TH6mfEK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "id": "Jnhi7gnWmfEK",
    "outputId": "1e45db9f-ea66-41b6-bec3-bd596eaac8be"
   },
   "outputs": [],
   "source": [
    "# detailed plot of regression performance\n",
    "\n",
    "# set some variables for creating a plot\n",
    "sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(10,5)})\n",
    "p = results.boxplot(column=\"prediction\", by=\"truth\", fontsize=15,\n",
    "                boxprops=dict(linestyle='-', linewidth=4),\n",
    "                medianprops=dict(linestyle='-', linewidth=4))\n",
    "p.set(xlabel=\"true value\", ylabel=\"predicted values\", title=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HtA5iho1mfEK"
   },
   "source": [
    "## Evaluate model\n",
    "\n",
    "Scikit-learn has a function to calculate an overall model [`score`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor.score:~:text=of%20the%20prediction.-,The%20coefficient%20of%20determination,score%20of%200.0.,-Parameters%3A) called the **coefficient of determination**:\n",
    "\n",
    "$\\huge R = 1-\\frac{\\sum_{i} (y_i - \\hat{y}_i)^2}{\\sum_{i} (y_i - \\bar{y})^2}$\n",
    "\n",
    "\n",
    "The part on the right divides the sum of squared differences between our predictions ($\\hat{y}_i$) and true values ($y_i$) by the sum of squared differences between true values ($y_i$) and the overall mean ($\\bar{y}_i$). This is like comparing our 'fancy' predictions with the simplest predictor possible: assign every new sample the mean value.\n",
    "\n",
    "So, this score essentially compares by well we do relative to just using the mean.\n",
    "\n",
    "**1 is good, 0 is bad**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5UWMuhvemfEK",
    "outputId": "1480cad3-b758-4052-99da-f991947fdaaa"
   },
   "outputs": [],
   "source": [
    "# score() returns the coefficient of determination of the prediction\n",
    "# note that our knn object has the training data already stored within it, so we only give it the test data here\n",
    "print(knn.score(X_test, Y_test))\n",
    "\n",
    "# or by hand, to show how it's calculated\n",
    "RSS = ((Y_test - knn.predict(X_test))** 2).sum()\n",
    "TSS = ((Y_test - Y_test.mean()) ** 2).sum()\n",
    "print(1-(RSS/TSS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHs_wv5ZmfEK"
   },
   "source": [
    "At least it's not zero?\n",
    "\n",
    "This score was for a single parameterization of the KNN model. We can try other parameterizations as mentioned above:\n",
    "\n",
    "1. different *K* values (number of neighbors)\n",
    "2. different weight functions (`uniform` or `distance`)\n",
    "3. different distance metrics (`l1` or `l2`)\n",
    "\n",
    "Experiment with some other parameters using the cell below which re-trains and re-scores the model. Do other values make a difference? What's the highest value you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K2XzDouSmfEL",
    "outputId": "8e527639-5f45-4035-aca2-109b53a4936b"
   },
   "outputs": [],
   "source": [
    "# Initialize, fit, predict\n",
    "knn = KNeighborsRegressor(n_neighbors=3, weights=\"uniform\", metric='l2')\n",
    "knn.fit(X_train, Y_train)\n",
    "print(knn.score(X_test, Y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SOy_K1kmfEL"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmY8JPkDmfEL"
   },
   "source": [
    "We will tune our model by finding the combination of parameter values that gives us the highest score!\n",
    "\n",
    "To automate this model tuning, let's put this code for model fitting and scoring into a single function `get_scores` which we can give a list of *K* values, a weighting function, and a distance function.\n",
    "\n",
    "This function will then return a list of scores, one for each value of *K*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Uzw44EUmfEL"
   },
   "outputs": [],
   "source": [
    "def get_scores(x_train,\n",
    "                x_test,\n",
    "                y_train,\n",
    "                y_test,\n",
    "                k_values,\n",
    "                weight_function,\n",
    "                distance_metric):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        k_values: a list of K nearest neighbors to use\n",
    "        w: the weight function to use\n",
    "        m: the distance metric to use\n",
    "    output:\n",
    "        a list of scores, one for each set of parameter values\n",
    "    \"\"\"\n",
    "\n",
    "    test_scores = []    # a list that will eventually contain all the model scores\n",
    "    for k in k_values:\n",
    "        # initialize\n",
    "        knn = KNeighborsRegressor(n_neighbors=k, weights=weight_function, metric=distance_metric)\n",
    "        # train model\n",
    "        knn.fit(x_train, y_train)\n",
    "        # score/test model\n",
    "        test_scores.append(knn.score(x_test, y_test))\n",
    "    return test_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uuC-GYcmfEL"
   },
   "source": [
    "Let's give `get_scores` a list of *K* values from 1 to 100.\n",
    "\n",
    "While we're at it, let's also compare the `uniform` and `distance` weighting functions.\n",
    "\n",
    "Thus, we will train and test 200 KNN models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "M5bWFAkRmfEL",
    "outputId": "7ae215bd-e750-48b0-89db-94d966d3aada"
   },
   "outputs": [],
   "source": [
    "k_values = [i for i in range(1,101)]    # a list containing values 1 through 100\n",
    "\n",
    "scores_uniform = get_scores(X_train, X_test, Y_train, Y_test, k_values, weight_function = \"uniform\", distance_metric = \"l2\")\n",
    "scores_distance = get_scores(X_train, X_test, Y_train, Y_test, k_values, weight_function = \"distance\", distance_metric = \"l2\")\n",
    "\n",
    "# plot results\n",
    "sns.set(style='whitegrid',font_scale=1.3, rc={'figure.figsize':(5,5)})\n",
    "p = sns.lineplot(x=k_values, y=scores_uniform, color='darkblue')\n",
    "sns.lineplot(x=k_values, y=scores_distance, color='darkred')\n",
    "p.set(xlabel=\"k neighbors\", ylabel=\"score\", title=\"red:distance, blue:uniform\")\n",
    "\n",
    "print(\"Best uniform score: \", max(scores_uniform))\n",
    "print(\"Best distance score: \", max(scores_distance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JsQp09uqmfEL"
   },
   "source": [
    "Let's use \"distance\" for the **weights** parameter.\n",
    "\n",
    "What about for the **metric** parameter? Use code from the cell above, and instead of varying the weight function, vary the distance metric to test out the L1 norm `l1` and the L2 norm `l2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "El-770YHmfEM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QrJx7gPmfEM"
   },
   "source": [
    "By constantly referring to test scores, we are using the data to make decisions about how to parameterize/configure our KNN model.\n",
    "\n",
    "---\n",
    "\n",
    "# Feature selection\n",
    "\n",
    "The success of KNN depends on the fact that samples that are nearby in feature space also have similar response values, such that we can use these KNN responses for accurate prediction. Stated another way, the values of the features are **correlated** with the reponse values. If this isn't the case, nearby neighbors might have wildly different reponse values, such that 'nearness' in feature space isn't informative for predicting reponse values, leading to inaccurate predictions!\n",
    "\n",
    "So far we've used all available features to make predictions, but some of these features may be useless.\n",
    "\n",
    "*At best*, including non-informative features makes our model more complex, less interpretable, more computationally costly to use and maintain (we will measure useless features for future data!). This is particularly problematic\n",
    "for neural networks, in which adding features can dramatically increase the number of parameters.\n",
    "\n",
    "*At worst*, non-informative features can ***reduce*** the model performance.\n",
    "\n",
    "Let's try to select only the more informative features by looking at the **Pearson correlation** between all of our variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yT9IFNawmfEM",
    "outputId": "7aababeb-cacc-477b-f8fc-47f88ce37375"
   },
   "outputs": [],
   "source": [
    "sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(15,15)})\n",
    "# calculate pearson correlation between each pair of features\n",
    "pcorr = red_df.corr(method=\"pearson\")\n",
    "\n",
    "# matrix is symmetric, upper right half same as lower left half\n",
    "# not necessary, but for viz purposes let's mask the upper right half\n",
    "mask = np.zeros_like(pcorr)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(pcorr, cmap=cmap, mask=mask, vmax=.3, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Skf5F0jmfEM"
   },
   "outputs": [],
   "source": [
    "# another way to qaulitatively inspect the relationships between our variables\n",
    "# warning: this function takes ~30 sec to run!\n",
    "#sns.pairplot(red_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTzUdRXomfEM"
   },
   "source": [
    "### How should we look at this plot? Which features should we select?\n",
    "While you ponder this, run the above cell that takes ~30 seconds to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNTiaWJRmfEN"
   },
   "outputs": [],
   "source": [
    "# Select features that have at least a +/- 20% correlation with the response.\n",
    "# This is an unsophisticated way to conduct feature selection but will do for today!\n",
    "# Domain expertise of the problem/system is very useful at this stage.\n",
    "\n",
    "red_df_selection = red_df[['volatile acidity', 'citric acid', 'sulphates', 'alcohol', 'quality']].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUx-lmItmfEN"
   },
   "source": [
    "Let's repeat the same analyses above on this new DataFrame with selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "WovGkCHwmfEN",
    "outputId": "8d638793-9559-46da-bc89-72164af4e831"
   },
   "outputs": [],
   "source": [
    "response = red_df_selection['quality']\n",
    "features = red_df_selection.drop(['quality'], axis=1)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features, response, test_size=0.30, random_state=11)\n",
    "\n",
    "scores_l2 = get_scores(X_train, X_test, Y_train, Y_test, k_values, weight_function = \"distance\", distance_metric =\"l2\")\n",
    "scores_l1 = get_scores(X_train, X_test, Y_train, Y_test, k_values, weight_function = \"distance\", distance_metric =\"l1\")\n",
    "\n",
    "sns.set(style='whitegrid',font_scale=1.3, rc={'figure.figsize':(5,5)})\n",
    "p = sns.lineplot(x=k_values, y=scores_l2, color='darkblue')\n",
    "sns.lineplot(x=k_values, y=scores_l1, color='darkred')\n",
    "p.set(xlabel=\"k neighbors\", ylabel=\"score\", title=\"red:L1, blue:L2\")\n",
    "\n",
    "print(\"Best score: \", max(scores_l1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4-THhOUmfEN"
   },
   "source": [
    "Feature selection makes a difference in overall predictions! Also, it can potentially affect the performance of other parameters (e.g. weight function, distance metric), so feature selection should really be done early on in the ML modeling process, during the exploratory data analysis.\n",
    "\n",
    "# Feature Normalization\n",
    "\n",
    "If one feature has a range of values between [0-15] (e.g. alcohol content) and another has a range only between [0-0.5] (e.g. chlorides), then the range of distances between test and training samples will also be different for these two features, such that some features will contribute more or less to the regression/classification model.\n",
    "\n",
    "This essentially means we aren't using all the data available to us!\n",
    "\n",
    "We fix this by ***normalizing*** our features such that they are scaled similarly, with similar distributions and ranges.\n",
    "\n",
    "Let's first use the `DataFrame.describe()` function to get an idea of how the range of each selected feature varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "CJEoFty0mfEN",
    "outputId": "d9556107-4d3a-43a4-f4e8-472fb7cbdb75"
   },
   "outputs": [],
   "source": [
    "features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4356URDOmfEN"
   },
   "source": [
    "\"Skewed\" distributions can have a similar impact on regression/classification, which we can inspect using `DataFrame.skew()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tsaCzXRlmfEN",
    "outputId": "82b83217-fb28-434f-d11f-54973d569abb"
   },
   "outputs": [],
   "source": [
    "features.skew()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwIxHdWOmfEO"
   },
   "source": [
    "To normalize, we'll use the standard Z score such that a feature's value will be transformed as $\\Large x' = \\frac{(x - \\mu)}{\\sigma}$.\n",
    "\n",
    "Another option is the min-max normalization $\\Large x' = \\frac{x - min(x)}{max(x) - min(x)}$\n",
    "\n",
    "### Which one do you think we should use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEOChFUtmfEO"
   },
   "outputs": [],
   "source": [
    "# Initiate the scaler\n",
    "scaler = StandardScaler()\n",
    "# another option\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "# Fit/transform to all numeric data\n",
    "features_transformed = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5gG9Wmk8mfEO"
   },
   "source": [
    "Let's confirm our features have actually been transformed as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "id": "6Dr9EjPXmfEO",
    "outputId": "6ee6b288-e6b7-439f-b346-382bee4de060"
   },
   "outputs": [],
   "source": [
    "sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(8,8)})\n",
    "pd.DataFrame(features_transformed).hist(bins=20,color='darkblue' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lsj0PxuomfEO"
   },
   "source": [
    "Let's rerun the regression with the transformed features to see if it increased scores. This is all the same code, except here we're using the `features_transformed` variable, not `features` as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "GxxGx-D2mfEO",
    "outputId": "6fbdf0c7-a7a2-461b-c917-2653d5189512"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(features_transformed, response, test_size=0.30, random_state=11)\n",
    "\n",
    "scores_l2 = get_scores(X_train, X_test, Y_train, Y_test, k_values, weight_function = \"distance\", distance_metric = \"l2\")\n",
    "scores_l1 = get_scores(X_train, X_test, Y_train, Y_test, k_values, weight_function = \"distance\", distance_metric = \"l1\")\n",
    "\n",
    "sns.set(style='whitegrid',font_scale=1.3, rc={'figure.figsize':(5,5)})\n",
    "p = sns.lineplot(x=k_values, y=scores_l2, color='darkblue')\n",
    "sns.lineplot(x=k_values, y=scores_l1, color='darkred')\n",
    "p.set(xlabel=\"k neighbors\", ylabel=\"score\", title=\"red:L1, blue:L2\")\n",
    "\n",
    "print(\"Best score: \", max(scores_l1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_h1fSpGzmfEP"
   },
   "source": [
    "We get a score of ~0.5 with the L1 norm and using ~15 neighbors! A pretty nice increase from our initial score of ~0.17 above.\n",
    "\n",
    "Using code from above, we can again visualize the true and predicted values within the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561
    },
    "id": "EGOypPaZmfEP",
    "outputId": "daa1c7d9-4d86-479b-fe1f-a9d4f8cdccad"
   },
   "outputs": [],
   "source": [
    "knn.fit(X_train, Y_train)\n",
    "\n",
    "tmp = pd.DataFrame.from_dict({\"truth\" : Y_test, \"prediction\" : knn.predict(X_test)})\n",
    "sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(10,5)})\n",
    "p = tmp.boxplot(column=\"prediction\", by=\"truth\", fontsize=15,\n",
    "                boxprops=dict(linestyle='-', linewidth=4),\n",
    "                medianprops=dict(linestyle='-', linewidth=4))\n",
    "p.set(xlabel=\"true value\", ylabel=\"predicted values\", title=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zwSBILx3mfEP"
   },
   "source": [
    "Looks like we still systematically overestimate values of bad wines and underestimate values of good wines.\n",
    "\n",
    "----\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOUVzNLYmfEP"
   },
   "source": [
    "# K-Nearest Neighbors Classification\n",
    "Let's take our quality scores and convert them to labels to see if we can classify a wine as \"poor\", \"average\", or \"excellent\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Mgzy7gAmfEP"
   },
   "outputs": [],
   "source": [
    "bins = [0, 4, 6, 10]\n",
    "labels = [\"poor\", \"average\", \"excellent\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRgg4C_zmfEP"
   },
   "source": [
    "We will use these numerical boundaries in `bins` to assign 1 of 3 labels to each wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "TzquWdmAmfEP",
    "outputId": "dc43ebd1-43dd-4e57-b1aa-fab2e86175c6"
   },
   "outputs": [],
   "source": [
    "# create a new 'quality_label' column by binning 'quality' into three categories\n",
    "red_df_selection.loc[:,'quality_label'] = pd.cut(red_df_selection['quality'], bins=bins, labels=labels)\n",
    "\n",
    "# we no longer need the 'quality' column\n",
    "red_df_selection.drop('quality', axis =1, inplace = True)\n",
    "red_df_selection.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAETTg1SmfEP"
   },
   "source": [
    "Do any of our features have information about our quality label? Type a feature into the `y=` argument of the `sns.boxplot` function. Options include those in the header we just printed above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "m6D99Ru7mfEQ",
    "outputId": "974f4fc6-5d43-4cad-fc4d-2cbb1eb16032"
   },
   "outputs": [],
   "source": [
    "sns.set(style='white',font_scale=1.3, rc={'figure.figsize':(5,5)})\n",
    "sns.boxplot(x='quality_label',y='alcohol', data=red_df_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbtm3GvjmfEQ"
   },
   "source": [
    "Here, I'm taking all the code from above and condensing it into one cell to see how classification performance changes with the number of K neighbors used.\n",
    "\n",
    "We will use scikit-learn's `KNeighborsClassifier` instead of `KNeighborsRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "9iGawSIfmfEQ",
    "outputId": "eec5d642-11b2-45f5-eb7a-0f9cd99fe5ee"
   },
   "outputs": [],
   "source": [
    "response = red_df_selection['quality_label']\n",
    "features = red_df_selection.drop(['quality_label'], axis=1)\n",
    "\n",
    "# feature normalization\n",
    "scaler = StandardScaler()\n",
    "features_transformed = scaler.fit_transform(features)\n",
    "\n",
    "# split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(features_transformed, response, test_size=0.30, random_state=11)\n",
    "\n",
    "# create function to test the KNN classifier across a range of K values\n",
    "# note that this uses the 'accuracy_score' for classification, instead of 'score' as used above for regression\n",
    "# the 'accuracy_score' is simply the fraction of correct predictions\n",
    "def knn_classification_iterate(x_train, x_test, y_train, y_test, k_values, w, m):\n",
    "    test_scores = []\n",
    "    # For each k\n",
    "    for k in k_values:\n",
    "        # Initialize, fit, predict\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, weights=w, metric=m)\n",
    "        knn.fit(x_train, y_train)\n",
    "        pred_knn = knn.predict(x_test)\n",
    "        accuracy_score(y_test, pred_knn)\n",
    "        test_scores.append(accuracy_score(Y_test, pred_knn))\n",
    "    return test_scores\n",
    "\n",
    "# Use the above function\n",
    "# note with KNN we may have ties, i.e. a test sample's 3 nearest neighbors are a poor, average, and excellent wine\n",
    "# 'breaking' these ties is a bit arbitrary, we will ignore this for now but in practice you should consider this!\n",
    "scores_l1 = knn_classification_iterate(X_train, X_test, Y_train, Y_test, k_values, w = \"distance\", m=\"l1\")\n",
    "\n",
    "# plot accuracy score results\n",
    "sns.set(style='whitegrid',font_scale=1.3, rc={'figure.figsize':(5,5)})\n",
    "p = sns.lineplot(x=k_values, y=scores_l1, color='darkblue')\n",
    "p.set(xlabel=\"k neighbors\", ylabel=\"accuracy\", title=\"\")\n",
    "\n",
    "print(\"Best score: \", max(scores_l1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URx7d0czmfEQ"
   },
   "source": [
    "Greater than 88% accuracy looks nice!\n",
    "\n",
    "However, this is a little misleading. For instance, let's revisit the distribution of quality scores in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GG7aZyISmfEQ",
    "outputId": "8bab35a5-ec37-40ca-8f9e-3369f5c61a8e"
   },
   "outputs": [],
   "source": [
    "red_df_selection['quality_label'].value_counts(normalize=True)\n",
    "Y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khWusOWqmfEQ"
   },
   "source": [
    "**Most** of our wines are 'average', such we could build a model without any features, labelling every single new sample as 'average', and our accuracy would be $\\frac{395}{395+65+20} = 0.823$\n",
    "\n",
    "Our accuracy scores of ~0.88 are certainly better, but this extremely simple approach already get us most of the way there...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHa_91M3mfEQ"
   },
   "source": [
    "Alternatively, say we are very interested in predicting 'excellent' wines and we are concerned about our 'excellent' predictions being accurate. The simple approach would be label each new wine as excellent, which would create a bad model with $\\frac{65}{395+65+20} = 0.135$ accuracy. How does the accuracy our our 'excellent' predictions compare to this?\n",
    "\n",
    "Let's create a [**confusion matrix**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html), where rows are the true labels and columns are the predicted labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ai7myh3zmfER",
    "outputId": "1828d778-eb10-4937-b523-988be9bb8227"
   },
   "outputs": [],
   "source": [
    "# make a classifier that's optimized according to our results above, with k=18 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=18, weights='distance', metric='l1')\n",
    "knn.fit(X_train, Y_train)\n",
    "pred_knn_for_best_k = knn.predict(X_test)\n",
    "\n",
    "# give confusion_matrix true values and predicted values to make a table\n",
    "cm = confusion_matrix(Y_test, pred_knn_for_best_k)\n",
    "\n",
    "# print the matrix\n",
    "names = [\"average\", \"excellent\", \"poor\"]\n",
    "print(pd.DataFrame(cm, index=names, columns=names))\n",
    "# rows are true labels, columns are predicted labels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoKUG3lEmfER"
   },
   "source": [
    "The accuracy we calculated above divides the sum of all the entries in the diagonal of this matrix (true positives) by all entries in the matrix (true positives and false positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "riW6UEfymfER",
    "outputId": "2ec4add8-73f7-4243-8fd7-390870ffaddd"
   },
   "outputs": [],
   "source": [
    "diagonal = (383+39+1)\n",
    "all = (383+39+1+12+26+19)\n",
    "\n",
    "diagonal/all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qF-tFDFlmfER"
   },
   "source": [
    "Of our $39+12=51$ excellent predictions (sum of entries in the 'excellent' column), $\\frac{39}{51} = 0.76$ are correct, which is much better than ~$13$% from randomly guessing based on the proportion of excellent wines in the training data. Note that we missed 26 'excellent' wines that our classifier throught were 'average'.\n",
    "\n",
    "These concepts are known as **specificity** and **recall**, and will be covered in more detail later in the workshop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IuNZhHAlmfER"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "74a539bb1a30b12322b0673657a3362ba97f8c77e4e9b57fd18ae9f35996046a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

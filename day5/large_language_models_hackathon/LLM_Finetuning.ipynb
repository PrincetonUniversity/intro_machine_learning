{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Fine-tuning\n",
        "\n",
        "- Instructor: Jake Snell\n",
        "- Date: January 23, 2024\n",
        "\n",
        "The contents of this tutorial are based on the following guide:\n",
        "- https://huggingface.co/docs/transformers/v4.37.0/tasks/language_modeling"
      ],
      "metadata": {
        "id": "9CeMVZyKbGbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Before beginning, be sure to select \"Runtime > Change runtime type > T4 GPU\". This will make fine-tuning a lot faster."
      ],
      "metadata": {
        "id": "_4tnLdZvex2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] datasets evaluate accelerate -U"
      ],
      "metadata": {
        "id": "eqgbwUVwVu7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Text Generation\n",
        "\n",
        "First, we need to download a LLM from HuggingFace. Here we use GPT-2, but feel free to experiment with your own choice of LLM by browsing https://huggingface.co/models?pipeline_tag=text-generation&sort=trending."
      ],
      "metadata": {
        "id": "SR9NV8Q-f-mq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's grab GPT-2 from HuggingFace\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(\"cuda:0\")"
      ],
      "metadata": {
        "id": "FhLo3HjIek-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's get the corresponding tokenizer as well\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "hIkJnA_8g0Qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's tokenize a sample sentence\n",
        "tokenized_sentence = tokenizer(\"One good tokenizer is worth more than a hundred bad ones.\")\n",
        "tokenized_sentence"
      ],
      "metadata": {
        "id": "dXbjKmSTg5Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**: Based on the tokenizer output, is your tokenizer a character-level, word-level, or subword-level tokenizer? How can you tell?"
      ],
      "metadata": {
        "id": "ASVY656BhTpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Exercise.** There are several methods for sampling a text sequence from a language model. Using the guide at https://huggingface.co/blog/how-to-generate, choose at least 2 sampling methods and implement them. Which technique generates higher quality text samples? What happens when you seed the text with different phrases, such as \"Wherefore art\", \"Four score and seven\", etc. Does the output match what you would expect?"
      ],
      "metadata": {
        "id": "Kvv4_ykefRA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to sample from the model here!"
      ],
      "metadata": {
        "id": "VWzDiClqfKBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Basic Fine-tuning\n",
        "\n",
        "Now we will download a small dataset to fine-tune our LLM. We will use the [TinyShakespeare](https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt) dataset here, but feel free to find your own dataset on HuggingFace: https://huggingface.co/datasets?sort=trending. If you do, be sure to choose one of the filters under \"Natural Language Processing\" so you get a text dataset.\n"
      ],
      "metadata": {
        "id": "OZ6wFrqSf36B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg2KYgkaVZk_"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Trelis/tiny-shakespeare\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "pbDKGeelVkaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quick check**: Verify that the text in the dataset is what you expect. Based on the original [TinyShakespeare](https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt) file, what strategy do you think was used to split into train and test? Would you have used this strategy, or something else?"
      ],
      "metadata": {
        "id": "ltqYqHV2ittg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we will need to tokenize this dataset using our tokenizer\n",
        "tokenized_dataset = dataset.map(\n",
        "    lambda example: tokenizer(example[\"Text\"]),\n",
        "    batched=True,\n",
        "    num_proc=4,\n",
        "    remove_columns=dataset[\"train\"].column_names\n",
        ")"
      ],
      "metadata": {
        "id": "ZsbFeyu9ajmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "CufJ3PKvbe0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, we will need to split the rows into blocks\n",
        "block_size = 128\n",
        "\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "    # customize this part to your needs.\n",
        "    if total_length >= block_size:\n",
        "        total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of block_size.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "NeSHOxayb0NW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_dataset = tokenized_dataset.map(group_texts, batched=True, num_proc=4)"
      ],
      "metadata": {
        "id": "4VqF4oR7ffjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuning_dataset"
      ],
      "metadata": {
        "id": "cSZBzw0mfjO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question:** Why is the number of rows different between `tokenized_dataset` and `finetuning_dataset`? Given the number of rows in a split from `tokenized_dataset`, can you write down an expression for the number of rows in the corresponding `finetuning_dataset` split?"
      ],
      "metadata": {
        "id": "4DScOzSeimSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we set up the data collator to pass into the training loop\n",
        "from transformers import DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "Sozp6ERKgkD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"my_awesome_shakespeare_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=finetuning_dataset[\"train\"],\n",
        "    eval_dataset=finetuning_dataset[\"test\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "pKDYb1JJfqc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we evaluate perplexity\n",
        "import math\n",
        "\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ],
      "metadata": {
        "id": "t-jxQZZ6gPmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Modify the code above to experiment with different learning rates, weight decay, and/or number of epochs.\n",
        "\n",
        "1. How do these choices affect training loss and validation loss? Which fine-tuning strategy is best?\n",
        "2. Use your sampling strategies from Part 1 above to sample from your fine-tuned model. How do the samples compare?"
      ],
      "metadata": {
        "id": "CAnO64bRkXCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: PEFT\n",
        "\n",
        "Another approach to fine-tuning is known as Parameter Efficient Fine-tuning (PEFT). See slides for a diagram of LoRA (Hu et al., 2021)\n",
        "\n",
        "**Exercise:** Use the HuggingFace [PEFT Guide](https://huggingface.co/docs/peft/quicktour) as a base to implement LoRA or the PEFT technique of your choice. Fine-tune your original LLM using PEFT, being sure to record training loss, validation loss. After PEFT fine-tuning is complete, generate some samples from your model.\n",
        "\n",
        "1. How do the training/validation losses and generated samples compare to your model from Part 2? Which model is better, in your opinion?\n",
        "2. How does the time taken for fine-tuning differ between ordinary fine-tuning and PEFT?\n",
        "3. What are some benefits of PEFT relative to ordinary fine-tuning? Which technique would you recommend to use in practice?"
      ],
      "metadata": {
        "id": "Cd3YKiTHlJR-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework (Optional)\n",
        "\n",
        "- Now that you know how to fine-tune a model for text generation, choose another text-based task. You could choose translation, text classification, or anything else that takes text as input. Fine-tune the LLM of your choice on this new task. How well does it perform? Did the promise of \"foundation model + adaptation\" live up to what you expected, or is there something still to be desired?\n",
        "- For a more in-depth look at transformers, you can check out [\"The Annotated Transformer\" tutorial](https://nlp.seas.harvard.edu/annotated-transformer/) by Sasha Rush. It covers masking and positional encoding in more detail, and also discusses advanced topics such as label smoothing and learning rate scheduling.\n",
        "\n",
        "# Thank you for joining us this Wintersession! We wish you the best of luck on your machine learning journey!\n",
        "\n",
        "If you have any questions or comments, please reach out to me at <js2523@princeton.edu>."
      ],
      "metadata": {
        "id": "LDh_YQW1ju5q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2cg59FkCj2NY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
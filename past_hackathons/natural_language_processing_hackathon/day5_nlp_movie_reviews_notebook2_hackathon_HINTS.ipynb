{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgyIBfhW3MOO3ltL5zC8DS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrincetonUniversity/intro_machine_learning/blob/main/day5/natural_language_processing_hackathon/day5_nlp_movie_reviews_notebook2_hackathon_HINTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to Machine Learning  \n",
        "**Natural Language Processing Hackathon: Notebook 2 HINTS   \n",
        "Wintersession  \n",
        "Tuesday, January 24, 2023**"
      ],
      "metadata": {
        "id": "MH7MrrKyZ3dQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The material here is based on Chapter 8 of \n",
        "Machine Learning with PyTorch and Scikit-Learn by Sebastian Raschka, Yuxi (Hayden) Liu, Vahid Mirjalili and Dmytro Dzhulgakov. The book is available via the PU library.\n",
        "\n",
        "In this notebook we are going to work with a dataset of 50,000 movie reviews from the Internet Movie Database (IMDb) and build a predictor that can distinguish between positive and negative reviews."
      ],
      "metadata": {
        "id": "W51U-7ZW4sNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import textwrap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "UuDdLpWUaBRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the data set:"
      ],
      "metadata": {
        "id": "wjO7F84nz99c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoSng-U6VyvC"
      },
      "outputs": [],
      "source": [
        "!wget https://tigress-web.princeton.edu/~jdh4/movie_data.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in the CSV file and print the first 5 rows of the Pandas dataframe:"
      ],
      "metadata": {
        "id": "peptRcYAdrSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "DuYihEqqcBwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the number of total rows and the data types:"
      ],
      "metadata": {
        "id": "rlcaf5fad1VT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "7tK0-ZCLdQVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check for class imbalance:"
      ],
      "metadata": {
        "id": "js0X9iZkda1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"sentiment\"].value_counts()"
      ],
      "metadata": {
        "id": "yvB3XKdudStC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classes are balanced so we do not need to worry about imbalance. Next, let's print some reviews to get a sense of the content."
      ],
      "metadata": {
        "id": "4uycrR1jeHBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_reviews_and_sentiment(d, start_index=42, num=3, width=80):\n",
        "    wrapper = textwrap.TextWrapper(width=width, break_long_words=False, break_on_hyphens=False)\n",
        "    for i in range(start_index, start_index + num):\n",
        "        print(wrapper.fill(str(d.loc[i][\"review\"])))\n",
        "        print('------------')\n",
        "        print(f'Sentiment: {d.loc[i][\"sentiment\"]}\\n')"
      ],
      "metadata": {
        "id": "NVoLU81BcQtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_reviews_and_sentiment(df, start_index=42, num=2)"
      ],
      "metadata": {
        "id": "cyhpu6ycjSNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change the value of idx to vary that amount of train and test data. The default value is 25000 or a 50/50 split."
      ],
      "metadata": {
        "id": "yN0XyTfcggrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessor(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', ''))\n",
        "    return text"
      ],
      "metadata": {
        "id": "YSqs-9TYhKt6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Via the first regex, <[^>]*>, in the preceding code section, we tried to remove all of the HTML markup from the movie reviews. Although many programmers generally advise against the use of regex to parse HTML, this regex should be sufficient to clean this particular dataset. Since we are only interested in removing HTML markup and do not plan to use the HTML markup further, using regex to do the job should be acceptable. However, if you prefer to use sophisticated tools for removing HTML markup from text, you can take a look at Pythonâ€™s HTML parser module, which is described at https://docs.python.org/3/library/html.parser.html. After we removed the HTML markup, we used a slightly more complex regex to find emoticons, which we temporarily stored as emoticons. Next, we removed all non-word characters from the text via the regex [\\W]+ and converted the text into lowercase characters."
      ],
      "metadata": {
        "id": "UhLHT8pu5uWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['review'] = df['review'].apply(preprocessor)"
      ],
      "metadata": {
        "id": "hDEbzfOahQOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_reviews_and_sentiment(df, start_index=42, num=2)"
      ],
      "metadata": {
        "id": "OI-4WUWUimJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a train-test split:"
      ],
      "metadata": {
        "id": "GZwH1OQLkBKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 25000\n",
        "X_train = df.loc[:idx - 1, 'review'].values\n",
        "y_train = df.loc[:idx - 1, 'sentiment'].values\n",
        "X_test  = df.loc[idx:, 'review'].values\n",
        "y_test  = df.loc[idx:, 'sentiment'].values"
      ],
      "metadata": {
        "id": "kOOBt1t4ccFx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try using the word counts as the features to get started:"
      ],
      "metadata": {
        "id": "J97KRI7pmnbS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(use_idf=False, norm=None, smooth_idf=False)\n",
        "word_counts = tfidf.fit_transform(X_train)"
      ],
      "metadata": {
        "id": "yhgfDr2OpreS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(word_counts)"
      ],
      "metadata": {
        "id": "Fxaw2kjoq_YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts.shape"
      ],
      "metadata": {
        "id": "5H1exZeIqP2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(tfidf.vocabulary_.items())[:10]"
      ],
      "metadata": {
        "id": "QJt0JBNZnzgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.loc[1][\"review\"])"
      ],
      "metadata": {
        "id": "xUqo7SwsseM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_counts[1,:])"
      ],
      "metadata": {
        "id": "ljeEWKf4sOYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf.vocabulary_[\"window\"]"
      ],
      "metadata": {
        "id": "XJctczjUpLn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression(C=1.0, solver='liblinear')\n",
        "clf = clf.fit(word_counts, y_train)"
      ],
      "metadata": {
        "id": "oWnjcA5wgz14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy on the test set is:"
      ],
      "metadata": {
        "id": "AB1o6eeOqBna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf.score(tfidf.transform(X_test), y_test)"
      ],
      "metadata": {
        "id": "UUZ9youasypj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the .transform() method was applied to the test set while .fit_transform() was applied to the train set. In this notebook we only worked with unnormalized word counts. We did nothing with stop-words, stemming, inverse document frequency weighting, n-grams, etc. The full solution in the next notebook uses a Pipeline to tryout various combinations of these choices to find the best one."
      ],
      "metadata": {
        "id": "fnBoPcDfqPpf"
      }
    }
  ]
}